{"posts":[{"title":"kafka 2.x 部署","content":"环境 centos7.x | jdk1.8.0_281 | kafka 2.12 附录一: zookeeper 完整配置内容; 附录二:kafka 完整配置内容 kafka 官网地址 https://kafka.apache.org/downloads 安装 本文以kafka_2.12-2.8.2版本为例, 使用 kafka 自带zookeeper, kafka下载地址, 默认读者已安装 jdk1.8 下载 wget https://archive.apache.org/dist/kafka/2.8.2/kafka_2.12-2.8.2.tgz 解压 tar -zxvf kafka_2.12-2.8.2.tgz zookeeper 配置 创建myid 内容为 1 zookeeper 集群, myid 按顺序编号 (例: 第一台 zookeeper为1, 第二台为2, 以此类推) vi /data1/data/zookeeper/myid 修改配置文件 vi kafka_2.12-2.8.2/config/zookeeper.properties 修改 dataDir 路径改为自己myid的路径 dataDir=/data1/data/zookeeper/ 新增以下内容 server.1 为第一台的 IP, 与myid相对应, server.2 依次类推 tickTime=2000 initLimit=10 syncLimit=5 server.1=10.1.5.15:2888:3888 server.2=10.1.5.16:2888:3888 server.3=10.1.5.31:2888:3888 kafka 配置 修改配置文件 vi kafka_2.12-2.8.2/config/server.properties 修改节点信息 1、修改broker.id 按照顺序编号, 1、2 以此类推 broker.id=1 2、修改 listeners listeners=PLAINTEXT://10.1.5.15:9092 3、参数优化 num.network.threads=16 num.io.threads=160 log.flush.interval.messages=10000 log.flush.interval.ms=1000 log.retention.hours=72 4、修改zookeeper.connect zookeeper.connect=10.1.5.15:2181,10.1.5.16:2181,10.1.5.31:2181,10.1.5.32:2181 5、新增内容 host.name为本机IP auto.create.topics.enable=false delete.topic.enable=true message.max.bytes=10485760 compression.type=producer host.name=10.1.5.15 启动服务 启动zookeeper 先启动zookeeper, 启动后jps命令查看进程, QuorumPeerMain 存活,且日志logs/zookeeper.out 没有错误 ./bin/zookeeper-server-start.sh -daemon ./config/zookeeper.properties 启动kafka 启动后jps命令查看进程, kafka 存活,且日志logs/server.log 没有错误 ./bin/kafka-server-start.sh -daemon ./config/server.properties 停止服务 停止 zookeeper ./bin/zookeeper-server-stop.sh 停止 kafka ./bin/kafka-server-stop.sh 附录一: # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the &quot;License&quot;); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # the directory where the snapshot is stored. dataDir=/data2/data/zookeeper2 # the port at which the clients will connect clientPort=2181 # disable the per-ip limit on the number of connections since this is a non-production config #maxClientCnxns=0 # Disable the adminserver by default to avoid port conflicts. # Set the port to something non-conflicting if choosing to enable this admin.enableServer=false # admin.serverPort=8080 tickTime=2000 initLimit=10 syncLimit=5 server.1=10.1.5.15:2888:3888 server.2=10.1.5.16:2888:3888 server.3=10.1.5.31:2888:3888 server.4=10.1.5.32:2888:3888 server.5=10.1.5.46:2888:3888 server.6=10.1.5.47:2888:3888 server.7=10.1.5.48:2888:3888 server.8=10.1.5.64:2888:3888 附录二: # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the &quot;License&quot;); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # see kafka.server.KafkaConfig for additional details and defaults ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=1 listeners=PLAINTEXT://10.1.5.15:9092 ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 #listeners=PLAINTEXT://:9092 # Hostname and port the broker5will advertise to producers and consumers. If not set, # it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=16 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=160 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma separated list of directories under which to store log files log.dirs=/data1/data/kafka2,/data2/data/kafka2,/data3/data/kafka2,/data4/data/kafka2,/data5/data/kafka2,/data6/data/kafka2,/data7/data/kafka2,/data8/data/kafka2,/data9/data/kafka2 # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot; # For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=72 # A size-based retention policy for logs. Segments are pruned from the log unless the remaining # segments drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;. # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=10.1.5.15:2181,10.1.5.16:2181,10.1.5.31:2181,10.1.5.32:2181,10.1.5.46:2181,10.1.5.47:2181,10.1.5.48:2181 # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=18000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 auto.create.topics.enable=false delete.topic.enable=true message.max.bytes=10485760 compression.type=producer host.name=10.1.5.15 ","link":"https://ibiji.top/post/1694484201386/"},{"title":"CentOS7 部署 SuperSet","content":"Superset 安装 安装 anaconda 1、安装依赖 sudo yum install bzip2 2、下载安装包 wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh 3、安装Anaconda sudo bash Anaconda3-2021.05-Linux-x86_64.sh 提示信息一： In order to continue the installation process, please review the license agreement. Please, press ENTER to continue &gt;&gt;&gt; 回车后继续安装过程； 提示信息二： Do you accept the license terms? [yes|no] [no] &gt;&gt;&gt; yes 阅读协议后，输入yes确认接受许可协议； 提示信息三： 确认Anaconda的安装位置 Anaconda3 will now be installed into this location: /root/anaconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below 提示信息四： 修改.bashrc脚本将 Anaconda目录添加到PATH中 Do you wish the installer to initialize Anaconda3 in your /root/.bashrc ? [yes|no] [no] &gt;&gt;&gt; yes 安装 superset 1、安装依赖 sudo yum install gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel 2、创建 conda 虚拟空间 conda create -n superset python=3.7 conda create -n superset python=3.7 ... zlib pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3 Proceed ([y]/n)? y Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # $ conda activate superset # To deactivate an active environment, use # $ conda deactivate 3、进入空间 conda activate superset 4、更新pip pip install --upgrade setuptools pip 5、安装superset pip install apache-superset 或指定douban源 pip install apache-superset -i https://pypi.douban.com/simple/ 6、初始化数据库superset db upgrade superset db upgrade logging was configured successfully INFO:superset.utils.logging_configurator:logging was configured successfully ... No PIL installation found ... 这里报错 No PIL installation found 执行 pip install pillow 后重新初始化数据库 pip install pillow Collecting pillow Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB) |████████████████████████████████| 3.1 MB 43 kB/s Installing collected packages: pillow Successfully installed pillow-8.4.0 7、创建管理员用户 export FLASK_APP=superset flask fab create-admin 根据提示输入用户名密码、名字和姓氏 Username [admin]: ... Recognized Database Authentications. Admin User admin created. 8、创建默认角色和权限 superset init 9、导入测试数据 superset load_examples 10、启动 superset run -h 0.0.0.0 -p 8880 配置数据源添加 clickhouse为例 进入数据库源界面 选择数据库类型 输入连接信息 #### sqlalchemy uri 填入驱动信息 clickhouse://&lt;用户名&gt;:&lt;密码&gt;@192.168.0.1:8123/&lt;表名&gt; 配置数据库 mysql pip install mysqlclient Presto pip install pyhive SparkSQL pip install pyhive ClickHouse pip install clickhouse-sqlalchemy 驱动版本需要 clickhouse-driver &gt; 0.2.0、clickhouse-sqlalchemy &gt; 0.1.5 错误解决 1、Was unable to import superset Error: No module named 'wtforms_json' pip install WTForms-JSON 2、Was unable to import superset Error: No module named 'flask_compress' pip install Flask-Compress 3、Was unable to import superset Error: No module named 'flask_migrate' pip install flask-migrate 4、Was unable to import superset Error: No module named 'celery' pip install celery 5、Was unable to import superset Error: No module named 'flask_talisman' pip install flask-talisaman 引用 - [1] anaconda 下载地址 - [2] superset 官方安装文档 ","link":"https://ibiji.top/post/1634912185907532/"},{"title":"LogStash Filter -- mutate","content":"过滤器指定配置 mutate { } 复制字段 copy mutate { copy =&gt; {&quot;name&quot; =&gt; &quot;name2&quot;} } 正则表达式替换 gsub mutate { gsub =&gt; [&quot;name&quot;,&quot;o&quot;,&quot;p&quot;] } 转变参数类型 convert mutate { convert =&gt; { &quot;name&quot; =&gt; &quot;string&quot; &quot;age&quot; =&gt; &quot;integer&quot; } } 大小写转换 lowercase&amp;uppercase mutate { #lowercase =&gt; [ &quot;name&quot; ] uppercase =&gt; [ &quot;name&quot; ] } 字段重命名 rename mutate { rename =&gt; {&quot;name&quot; =&gt; &quot;name3&quot;} } 除去字段值前后空格 strip mutate { strip =&gt; [&quot;name&quot;] } 更新字段值 update/replace replace 作用和 update 类似，但是当字段不存在的时候，它会起到 add_field 参数一样的效果，自动添加新的字段。 mutate { update =&gt; {&quot;name&quot; =&gt; &quot;li&quot;} } 移除字段 remove_field mutate { remove_field =&gt; [&quot;name&quot;] } 增加字段 add_field mutate { add_field =&gt; {&quot;testField1&quot; =&gt; &quot;0&quot;} add_field =&gt; {&quot;testField2&quot; =&gt; &quot;%{Field}&quot;} #引用 Field 中的值 } 执行次序 rename(event) if @rename update(event) if @update replace(event) if @replace convert(event) if @convert gsub(event) if @gsub uppercase(event) if @uppercase lowercase(event) if @lowercase strip(event) if @strip remove(event) if @remove split(event) if @split join(event) if @join merge(event) if @merge filter_matched(event) ","link":"https://ibiji.top/post/1632663907735209/"},{"title":"Hadoop 3.x web 界面 upload 文件 Couldn't upload the file","content":"&lt;!--安全认证初始化的类--&gt; &lt;property&gt; &lt;name&gt;hadoop.http.filter.initializers&lt;/name&gt; &lt;value&gt;org.apache.hadoop.security.HttpCrossOriginFilterInitializer&lt;/value&gt; &lt;/property&gt; &lt;!--是否启用跨域支持--&gt; &lt;property&gt; &lt;name&gt;hadoop.http.cross-origin.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--允许跨域访问的来源，如果有多个，用逗号(,)分隔--&gt; &lt;property&gt; &lt;name&gt;hadoop.http.cross-origin.allowed-origins&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--允许跨域的方法列表，如果有多个，用逗号(,)分隔--&gt; &lt;property&gt; &lt;name&gt;hadoop.http.cross-origin.allowed-methods&lt;/name&gt; &lt;value&gt;GET,POST,HEAD&lt;/value&gt; &lt;/property&gt; &lt;!--允许跨域的标头列表，如果有多个，用逗号(,)分隔--&gt; &lt;property&gt; &lt;name&gt;hadoop.http.cross-origin.allowed-headers&lt;/name&gt; &lt;value&gt;X-Requested-With,Content-Type,Accept,Origin&lt;/value&gt; &lt;/property&gt; &lt;!--预检请求可以缓存的秒数--&gt; &lt;property&gt; &lt;name&gt;hadoop.http.cross-origin.max-age&lt;/name&gt; &lt;value&gt;1800&lt;/value&gt; &lt;/property&gt; ","link":"https://ibiji.top/post/1616739950215654/"},{"title":"nginx + tomcat 集群测试","content":"运行环境| centos 7.0 | VMware Workstation 15 Pro | tomcat 8.5 | nginx 1.14.2 Tomcat集群 + Nginx负载均衡测试 测试命令： ab -n20000 -c1000 http://192.168.244.10/drp/index.jsp Nginx + Tomcat集群测试结果 Server Software: nginx/1.14.2 Server Hostname: 192.168.244.10 Server Port: 80 Document Path: /drp/index.jsp Document Length: 216 bytes Concurrency Level: 1000 Time taken for tests: 9.126 seconds Complete requests: 20000 Failed requests: 488 (Connect: 0, Receive: 0, Length: 488, Exceptions: 0) Write errors: 0 Non-2xx responses: 488 Total transferred: 8973152 bytes HTML transferred: 4308776 bytes Requests per second: 2191.63 [#/sec] (mean) Time per request: 456.282 [ms] (mean) Time per request: 0.456 [ms] (mean, across all concurrent requests) Transfer rate: 960.24 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 226 628.0 0 7000 Processing: 0 146 325.0 51 7122 Waiting: 0 146 325.0 51 7122 Total: 1 372 757.6 60 8422 Percentage of the requests served within a certain time (ms) 50% 60 66% 147 75% 260 80% 385 90% 1070 95% 1831 98% 3062 99% 3230 100% 8422 (longest request) Tomcat 集群测试结果 Server Software: Server Hostname: 192.168.244.11 Server Port: 8080 Document Path: /drp/index.jsp Document Length: 216 bytes Concurrency Level: 1000 Time taken for tests: 5.161 seconds Complete requests: 20000 Failed requests: 0 Write errors: 0 Total transferred: 8733153 bytes HTML transferred: 4397112 bytes Requests per second: 3875.41 [#/sec] (mean) Time per request: 258.037 [ms] (mean) Time per request: 0.258 [ms] (mean, across all concurrent requests) Transfer rate: 1652.56 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 153 282.6 70 1125 Processing: 17 96 51.3 85 2010 Waiting: 0 74 44.9 69 2004 Total: 28 250 294.5 167 2025 Percentage of the requests served within a certain time (ms) 50% 167 66% 186 75% 198 80% 210 90% 293 95% 1185 98% 1219 99% 1232 100% 2025 (longest request) ","link":"https://ibiji.top/post/1597299780863965/"},{"title":"Hive启动报错2","content":"启动Hive报错 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ Connecting to jdbc:hive2://master:10000 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:279) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:206) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:507) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1612) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1630) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:551) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3000) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1107) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:873) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489) (state=,code=0) Beeline version 1.2.2 by Apache Hive 进入Hive后 Connecting to jdbc:hive2://master:10000 Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0) Beeline version 1.2.2 by Apache Hive 0: jdbc:hive2://master:10000 (closed)&gt; 解决方法： HDFS 没有读写权限 配置文件中加入 &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; 对 hdfs /tmp 加入777 权限 hdfs dfs -chmod -R 777 /tmp/ ","link":"https://ibiji.top/post/1597299417459319/"},{"title":"Hive HBase 整合","content":"运行环境 | centos 7.0 | hbase 1.2.10 | hive 1.2.2 Hive只支持insert和delete操作，并不支持update操作，所以无法实施更新hive里的数据，而HBase正好弥补了这一点，所以在某些场景下需要将Hive和HBase整合起来一起使用。 Hive与HBase整合的实现是利用两者本身对外的API接口互相通信来完成的，其具体工作交由Hive的lib目录中的hive-hbase-handler-*.jar工具类来实现，通信原理如下图所示 Hive整合HBase后的使用场景： 通过Hive把数据加载到HBase中，数据源可以是文件也可以是Hive中的表。 通过整合，让HBase支持JOIN、GROUP等SQL查询语法。 通过整合，不仅可完成HBase的数据实时查询，也可以使用Hive查询HBase中的数据完成复杂的数据分析。 把HBASE_HOME/lib下所有的jar 复制到HIVE_HOME/lib/下 cp -n $HBASE_HOME/lib/* $HIVE_HOME/lib/ -n 表示对于目标路径下已经存在的文件，则不复制过去 修改hive-site.xml文件，增加HBase的ZooKeeper集群信息 &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; 指定 hbase 所使用的 zookeeper 集群的地址：默认端口是 2181，可以不写 set hbase.zookeeper.quorum=node01:2181,node02:2181,node03:2181; set zookeeper.znode.parent=/var/zookeeper/local; 指定 hbase 在 zookeeper 中使用的根目录 该路径在hbase-site.xml可以查询，你在搭建hbase集群的时候，这个参数是必须设置的 &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/var/zookeeper/local&lt;/value&gt; &lt;/property&gt; 把HIVE_HOME/lib/hive-hbase-handler-1.2.1.jar 复制到HBASE_HOME/lib/下 cp */hive-1.2.1/lib/hive-hbase-handler-1.2.1.jar */hbase/lib/ 启动HBase,创建HBase表&quot;student&quot; create 'student','info' put 'student','0001','info:name','Tom' put 'student','0001','info:age','18' put 'student','0002','info:name','Madeline' put 'student','0002','info:age','15' put 'student','0003','info:name','jed' put 'student','0003','info:age','16' put 'student','0004','info:name','olivia' put 'student','0004','info:age','18' put 'student','0005','info:name','sarah' put 'student','0005','info:age','19' put 'student','0006','info:name','xiaohong' put 'student','0006','info:age','19' put 'student','0007','info:name','zhangsan' put 'student','0007','info:age','20' put 'student','0008','info:name','wangwu' put 'student','0008','info:age','22' Hive中创建表结构 CREATE EXTERNAL TABLE hive_student_name (key string,name string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:name&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;student&quot;); CREATE EXTERNAL TABLE hive_student_age (key string,name string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:age&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;student&quot;); 通过Hive客户端可以查询该表的数据 ","link":"https://ibiji.top/post/1597299498833374/"},{"title":"cesium(笔记-5)","content":"运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro 记初步实现 cesium 通过鼠标完成模型创建 通过鼠标滑动实时抓取当前坐标与高度，动态刷新到左侧二维缩略图信息框中，用户可通过手动输入或鼠标实时获取坐标点，点击确认添加后，在指定坐标点生成对应模型 案例模型采用3D Tiles 及 GLTF 3D Tiles官方介绍 GLTF 官方介绍 3D Tiles介绍： 3D Tiles 是： Open（开放） Optimized for streaming and rendering（针对流和渲染进行了优化） Designed for 3D（专为3D设计） Interactive（交互式互动） Styleable（设置样式） Adaptable（适应性强） Flexible（灵活） Heterogeneous（异构的 Precise（精确） Temporal（时间动态 渲染 3D Tiles是基于状态，从UNLOADING开始，通过一系列的request，完成最初的数据加载过程，结束LOADING状态，进入Pocessing过程，也就是数据解析。数据解析完后进入READY状态，通过selectTile，最终调用Content对应的update方法，构造最终的drawcommand，加入渲染队列。当然，如果有需要释放的Tile，则在unloadTiles中处理。细心的人会发现Pocessing和Ready状态。最终调用的都是update方法。这里解释一下：3D Tiles中主要的数据部分就是glTF，而glTF也是基于状态管理的，无论是glTF的解析还是构造DrawCommand，只是state不同，都是在update方法中完成的。 GLTF介绍： gltf的核心是一个JSON文件。这个文件描述了3D场景的全部内容。它由场景结构本身的描述组成，由定义场景图的节点层次结构给出。场景中出现的三维对象是使用附着到节点上的网格定义的。材质定义对象的外观。动画描述了随着时间的推移，三维对象是如何转换的（例如，旋转到转换），而蒙皮定义了基于骨架姿势的对象几何体是如何变形的。摄影机描述渲染器的视图配置。 基础结构 scene：整个场景的入口点，由node构成图（树）结构（类似OSG的场景组织）组成 node：场景层级中的一个节点，可以包含位置变换，可以有子节点。同时，node通过指向mesh、camera、skin来描述node的形变 camera：定义渲染场景的视点配置 mesh：描述了在场景中出现的几何物体，并通过accessor（访问器）来访问其真实的几何数据，通过material（材质）来确定渲染外观 skin：蒙皮描述模型绑定到骨骼上的参数，用来实现骨骼动画，其真实数据也是通过访问器来获取的 animation：骨骼动画，描述某个节点怎么随着时间运动 accessor：访问器，定义了如何从二进制数据源中获取数据，在mesh、animation、skin中都会用到访问器。其指向buffer和bufferview，在这里面存着真正的几何数据 material：材质包含了定义物体模型外观的参数，特别是纹理参数 texture：包括采样器和图片，定义如何将纹理映射到模型 对外部数据的引用 二进制数据，如三维对象的几何图形和纹理，通常不包含在JSON文件中。相反，它们存储在专用文件中，JSON部分只包含指向这些文件的链接。这允许二进制数据以非常紧凑的形式存储，并且可以通过Web高效地传输。此外，数据可以直接存储在渲染器中，而不必解析、解码或预处理数据。 实现思路 需要获取坐标点与高度 屏幕坐标 var handler= new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas); handler.setInputAction(function (movement) { console.log(movement.position); }, Cesium.ScreenSpaceEventType.LEFT_CLICK); 世界坐标 var handler = new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas); handler.setInputAction(function (movement) { var position = viewer.scene.camera.pickEllipsoid(movement.position, viewer.scene.globe.ellipsoid); console.log(position); }, Cesium.ScreenSpaceEventType.LEFT_CLICK); 场景坐标 var handler = new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas); handler.setInputAction(function (movement) { var position = viewer.scene.pickPosition(movement.position); console.log(position); }, Cesium.ScreenSpaceEventType.LEFT_CLICK); 地标坐标 var handler = new Cesium.ScreenSpaceEventHandler(viewer.scene.canvas); handler.setInputAction(function (movement) { var ray=viewer.camera.getPickRay(movement.position); var position = viewer.scene.globe.pick(ray, viewer.scene); console.log(position); }, Cesium.ScreenSpaceEventType.LEFT_CLICK); 加载模型 var entity = viewer.entities.add({ name: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', position: position, orientation: orientation, model: { uri: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', } }); viewer.trackedEntity = entity; 完整代码实现 JS var viewer = new Cesium.Viewer('cesiumContainer', { imageryProvider: Cesium.createOpenStreetMapImageryProvider({ url: 'https://a.tile.openstreetmap.org/' }), shouldAnimate: false, infoBox: false, selectionIndicator: false, shadows: false, geocoder: false, homeButton: false, sceneModePicker: false, baseLayerPicker: false, navigationHelpButton: false, // animation: false, // timeline: false, fullscreenButton: false }); var scene = viewer.scene; // var handler = new Cesium.ScreenSpaceEventHandler(scene.canvas); handler.setInputAction(function (movement) { var cartesian = scene.camera.pickEllipsoid(movement.endPosition, ellipsoid); var ellipsoid = scene.globe.ellipsoid; if (cartesian) { var cartographic = ellipsoid.cartesianToCartographic(cartesian); var coords = 'lng' + Cesium.Math.toDegrees(cartographic.longitude).toFixed(6) + ', ' + 'lat' + Cesium.Math.toDegrees(cartographic.latitude).toFixed(6) + '，' + 'hig' + Math.ceil(viewer.camera.positionCartographic.height); console.log(&quot;--&gt;&quot; + coords) $(&quot;#val1&quot;).val(Cesium.Math.toDegrees(cartographic.longitude).toFixed(6)) $(&quot;#val2&quot;).val(Cesium.Math.toDegrees(cartographic.latitude).toFixed(6)) $(&quot;#val3&quot;).val(Math.ceil(viewer.camera.positionCartographic.height)) } else { // console.log(&quot;--&gt;&quot; + ERROR) } }, Cesium.ScreenSpaceEventType.MOUSE_MOVE); var params = { tx: $(&quot;#val1&quot;).val(), //模型中心X轴坐标（经度，单位：十进制度） ty: $(&quot;#val2&quot;).val(), //模型中心Y轴坐标（纬度，单位：十进制度） tz: 0, //模型中心Z轴坐标（高程，单位：米） rx: 0, //X轴（经度）方向旋转角度（单位：度） ry: 0, //Y轴（纬度）方向旋转角度（单位：度） rz: 0 //Z轴（高程）方向旋转角度（单位：度） }; function createModel() { console.log(&quot;------&gt;&quot; + $(&quot;#height&quot;).val()); // // viewer.entities.removeAll(); var position = Cesium.Cartesian3.fromDegrees($(&quot;#val1&quot;).val(), $(&quot;#val2&quot;).val(), $(&quot;#val3&quot;).val()); var heading = Cesium.Math.toRadians(135); var pitch = 0; var roll = 0; var hpr = new Cesium.HeadingPitchRoll(heading, pitch, roll); var orientation = Cesium.Transforms.headingPitchRollQuaternion(position, hpr); var entity = viewer.entities.add({ name: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', position: position, orientation: orientation, model: { uri: 'model/CesiumMilkTruck/CesiumMilkTruck.glb', } }); viewer.trackedEntity = entity; // 3D Tiles /*viewer.scene.globe.depthTestAgainstTerrain = true; // var url = &quot;model/daodan/tileset.json&quot;; var url = Cesium.IonResource.fromAssetId(1); var tileset = new Cesium.Cesium3DTileset({url: url}); var primitive1 = viewer.scene.primitives.add(tileset); primitive1.readyPromise.then(function (t) { console.log(&quot;====&gt;&quot;+$(&quot;#val1&quot;).val() + &quot;,&quot;+$(&quot;#val2&quot;).val()) var originalSphere = t.boundingSphere; var radius = originalSphere.radius; viewer.zoomTo(t, new Cesium.HeadingPitchRange(0.5, -0.5, radius * 4.0)); // var mx = Cesium.Matrix3.fromRotationX(Cesium.Math.toRadians(params.rx)); var my = Cesium.Matrix3.fromRotationY(Cesium.Math.toRadians(params.ry)); var mz = Cesium.Matrix3.fromRotationZ(Cesium.Math.toRadians(params.rz)); var rotationX = Cesium.Matrix4.fromRotationTranslation(mx); var rotationY = Cesium.Matrix4.fromRotationTranslation(my); var rotationZ = Cesium.Matrix4.fromRotationTranslation(mz); // var position = Cesium.Cartesian3.fromDegrees(params.tx, params.ty, $(&quot;#val3&quot;).val()); var position = Cesium.Cartesian3.fromDegrees($(&quot;#val1&quot;).val(), $(&quot;#val2&quot;).val(), $(&quot;#val3&quot;).val()); var m = Cesium.Transforms.eastNorthUpToFixedFrame(position); // Cesium.Matrix4.multiply(m, rotationX, m); Cesium.Matrix4.multiply(m, rotationY, m); Cesium.Matrix4.multiply(m, rotationZ, m); tileset._root.transform = m; }).otherwise(function (error) { var msg = JSON.stringify(error); console.log(&quot;msg -------------&gt;&quot; + msg); });*/ } HTML 代码 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no&quot;&gt; &lt;link href=&quot;css/widgets.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;title&gt;add model&lt;/title&gt; &lt;script src=&quot;https://cdn.bootcss.com/jquery/2.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;css/style.css&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;bootstrap/css/bootstrap.css&quot;/&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;js/jquery-3.4.1.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;bootstrap/js/bootstrap.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;js/Sandcastle-header.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;js/require.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;js/Cesium.js&quot;&gt;&lt;/script&gt; &lt;!-- 开发环境版本，包含了有帮助的命令行警告 --&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue/dist/vue.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html, body, #cesiumContainer { width: 100%; height: 100%; margin: 0; padding: 0; overflow: hidden; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;style&gt; @import url(css/bucket.css); #toolbar { background: rgba(42, 42, 42, 0.8); padding: 4px; border-radius: 4px; } #toolbar input { vertical-align: middle; padding-top: 2px; padding-bottom: 2px; } &lt;/style&gt; &lt;div id=&quot;cesiumContainer&quot; class=&quot;fullSize&quot;&gt;&lt;/div&gt; &lt;div id=&quot;loadingOverlay&quot;&gt;&lt;h1&gt;Loading...&lt;/h1&gt;&lt;/div&gt; &lt;div id=&quot;toolbar&quot;&gt; &lt;!--&lt;div&gt;Height&lt;/div&gt;--&gt; &lt;!--&lt;input type=&quot;range&quot; min=&quot;-100.0&quot; max=&quot;100.0&quot; step=&quot;1&quot; data-bind=&quot;value: height, valueUpdate: 'input'&quot;&gt;--&gt; &lt;!--&lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: height&quot;&gt;--&gt; &lt;!--&lt;input value=&quot;0&quot; id=&quot;height&quot; type=&quot;text&quot;&gt;--&gt; &lt;!--&lt;button onclick=&quot;createModel()&quot;&gt;添加&lt;/button&gt;--&gt; &lt;div class=&quot;panel panel-default&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt;车&lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;div&gt; &lt;img src=&quot;img/car.jpg&quot; style=&quot;height: 100px&quot;&gt; &lt;/div&gt; &lt;hr/&gt; &lt;!-- Single button --&gt; &lt;div class=&quot;btn-group&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-default dropdown-toggle&quot; data-toggle=&quot;dropdown&quot; aria-haspopup=&quot;true&quot; aria-expanded=&quot;false&quot;&gt; 参数 &lt;span class=&quot;caret&quot;&gt;&lt;/span&gt; &lt;/button&gt; &lt;ul class=&quot;dropdown-menu&quot;&gt; &lt;li&gt;&lt;a href=&quot;#&quot;&gt;经度&lt;input id=&quot;val1&quot; type=&quot;text&quot; style=&quot;color: #000000&quot;/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#&quot;&gt;纬度&lt;input id=&quot;val2&quot; type=&quot;text&quot; style=&quot;color: #000000&quot;/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#&quot;&gt;高度&lt;input id=&quot;val3&quot; type=&quot;text&quot; style=&quot;color: #000000&quot;/&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;button onclick=&quot;createModel()&quot;&gt;&lt;a&gt;确认添加&lt;/a&gt;&lt;/button&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 效果图 ","link":"https://ibiji.top/post/1597298807005150/"},{"title":"cesium(笔记-4)","content":"运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro 粒子系统是一种图形技术，可以模拟复杂的物理效果。粒子系统是小图像的集合，当它们一起观看时，会形成一个更复杂的“模糊”物体，如火、烟、天气或烟花fireworkds。通过使用诸如初始位置、速度和寿命等属性指定单个粒子的行为，可以控制这些复杂的效果。 粒子系统效应在电影和电子游戏中很常见。例如，为了表示飞机的损坏，技术艺术家可以使用粒子系统来表示飞机引擎上的爆炸，然后渲染不同的粒子系统，表示飞机坠毁时的烟雾轨迹。 Particle system basics 粒子系统基础 var particleSystem = viewer.scene.primitives.add(new Cesium.ParticleSystem({ image : '../../SampleData/smoke.png', imageSize : new Cesium.Cartesian2(20, 20), startScale : 1.0, endScale : 4.0, particleLife : 1.0, speed : 5.0, emitter : new Cesium.CircleEmitter(0.5), emissionRate : 5.0, modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0 })); Emitters 发射器 当粒子诞生时，其初始位置和速度矢量由ParticleEmitter控制。发射器将每秒生成一些粒子，由emissionRate参数指定，根据发射器类型用随机速度初始化。 BoxEmitter 盒形发射器 var particleSystem = scene.primitives.add(new Cesium.ParticleSystem({ image : '../../SampleData/smoke.png', color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.BoxEmitter(new Cesium.Cartesian3(5.0, 5.0, 5.0)), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0 })); CircleEmitter 圆形发射器 BoxEmitter在一个盒子内随机取样的位置初始化粒子，从六个盒子表面中的一个引导出来. CircleEmitter 圆形发射器 CircleEmitter在发射器上轴线方向上的圆形内的随机采样位置初始化粒子。 var particleSystem = scene.primitives.add(new Cesium.ParticleSystem({ image : '../../SampleData/smoke.png', color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.CircleEmitter(5.0), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0 })); ConeEmitter 锥形发射器 ConeEmitter在圆锥体的顶端初始化粒子，并以随机的角度引导它们离开圆锥体。 var particleSystem = scene.primitives.add(new Cesium.ParticleSystem({ image : '../../SampleData/smoke.png', color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.ConeEmitter(Cesium.Math.toRadians(30.0)), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0 })); SphereEmitter 球形发射器 SphereEmitter在球体内随机取样的位置初始化粒子，并将它们从球体中心向外引导。 var particleSystem = scene.primitives.add(new Cesium.ParticleSystem({ image : '../../SampleData/smoke.png', color: Cesium.Color.MAGENTA, emissionRate: 5.0, emitter: new Cesium.SphereEmitter(5.0), imageSize : new Cesium.Cartesian2(25.0, 25.0), modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()), lifetime : 16.0 })); Particle emission rate 粒子发射率 emissionRate控制每秒发射多少粒子，这会改变系统中粒子的密度。 指定一组突burst以在指定时间发射粒子burst（如上面的动画所示）。这会增加粒子系统的多样性或爆炸性。 bursts : [ new Cesium.ParticleBurst({time : 5.0, minimum : 300, maximum : 500}), new Cesium.ParticleBurst({time : 10.0, minimum : 50, maximum : 100}), new Cesium.ParticleBurst({time : 15.0, minimum : 200, maximum : 300}) ] Life of the particle and life of the system 粒子寿命和系统寿命 lifetime : 16.0, loop: false //要随机化每个粒子的输出，使用变量minimumParticleLife和maximumArticleLife。 minimumParticleLife: 5.0, maximumParticleLife: 10.0 Styling particles 样式化粒子 //Color 颜色 startColor : Cesium.Color.LIGHTSEAGREEN.withAlpha(0.7), endColor : Cesium.Color.WHITE.withAlpha(0.0), //Size 大小 minimumImageSize : new Cesium.Cartesian2(30.0, 30.0), maximumImageSize : new Cesium.Cartesian2(60.0, 60.0) //可以通过startScale和endscale属性在其生命周期中进行调整，以使粒子随时间增长或收缩。 startScale: 1.0, endScale: 4.0 //速度由speed或minimumSpeed和maximumSpeed控制。 minimumSpeed: 5.0, maximumSpeed: 10.0 通过应用更新函数，可以进一步自定义粒子系统。 对于重力、风或颜色更改等效果，它充当每个粒子的手动更新程序。 项目系统有一个updateCallback，它在模拟过程中修改粒子的属性。此函数采用粒子和模拟时间步骤。大多数基于物理的效果将修改速度矢量以改变方向或速度。下面是一个让粒子对重力作出反应的例子： var gravityVector = new Cesium.Cartesian3(); var gravity = -(9.8 * 9.8); function applyGravity(p, dt) { // Compute a local up vector for each particle in geocentric space. var position = p.position; Cesium.Cartesian3.normalize(position, gravityVector); Cesium.Cartesian3.multiplyByScalar(gravityVector, gravity * dt, gravityVector); p.velocity = Cesium.Cartesian3.add(p.velocity, gravityVector, p.velocity); } //该函数计算重力矢量，并使用重力加速度来改变粒子的速度。 将重力设置为粒子系统的updateFunction： updateCallback : applyGravity Positioning 定位 /* modelMatrix：将粒子系统从模型转换为世界坐标。 emitterModelMatrix：在粒子系统的局部坐标系中变换粒子系统发射器。 */ var entity = viewer.entities.add({ model : { uri : '../../SampleData/models/CesiumMilkTruck/CesiumMilkTruck-kmc.glb' }, position : Cesium.Cartesian3.fromDegrees(-75.15787310614596, 39.97862668312678) }); viewer.trackedEntity = entity; modelMatrix: entity.computeModelMatrix(time, new Cesium.Matrix4()) function computeEmitterModelMatrix() { hpr = Cesium.HeadingPitchRoll.fromDegrees(0.0, 0.0, 0.0, hpr); trs.translation = Cesium.Cartesian3.fromElements(-4.0, 0.0, 1.4, translation); trs.rotation = Cesium.Quaternion.fromHeadingPitchRoll(hpr, rotation); return Cesium.Matrix4.fromTranslationRotationScale(trs, emitterModelMatrix); } var particleSystem = viewer.scene.primitives.add(new Cesium.ParticleSystem({ image : '../../SampleData/smoke.png', startColor : Cesium.Color.LIGHTSEAGREEN.withAlpha(0.7), endColor : Cesium.Color.WHITE.withAlpha(0.0), startScale : 1.0, endScale : 4.0, particleLife : 1.0, minimumSpeed : 1.0, maximumSpeed : 4.0 imageSize : new Cesium.Cartesian2(25, 25), emissionRate : 5.0, lifetime : 16.0, modelMatrix : entity.computeModelMatrix(viewer.clock.startTime, new Cesium.Matrix4()) emitterModelMatrix : computeEmitterModelMatrix() })); 完整示例 var viewer = new Cesium.Viewer('cesiumContainer'); //Set the random number seed for consistent results. Cesium.Math.setRandomNumberSeed(3); //Set bounds of our simulation time var start = Cesium.JulianDate.fromDate(new Date(2015, 2, 25, 16)); var stop = Cesium.JulianDate.addSeconds(start, 120, new Cesium.JulianDate()); //Make sure viewer is at the desired time. viewer.clock.startTime = start.clone(); viewer.clock.stopTime = stop.clone(); viewer.clock.currentTime = start.clone(); viewer.clock.clockRange = Cesium.ClockRange.LOOP_STOP; //Loop at the end viewer.clock.multiplier = 1; viewer.clock.shouldAnimate = true; //Set timeline to simulation bounds viewer.timeline.zoomTo(start, stop); var viewModel = { emissionRate : 5.0, gravity : 0.0, minimumParticleLife : 1.2, maximumParticleLife : 1.2, minimumSpeed : 1.0, maximumSpeed : 4.0, startScale : 1.0, endScale : 5.0, particleSize : 25.0 }; Cesium.knockout.track(viewModel); var toolbar = document.getElementById('toolbar'); Cesium.knockout.applyBindings(viewModel, toolbar); var entityPosition = new Cesium.Cartesian3(); var entityOrientation = new Cesium.Quaternion(); var rotationMatrix = new Cesium.Matrix3(); var modelMatrix = new Cesium.Matrix4(); function computeModelMatrix(entity, time) { return entity.computeModelMatrix(time, new Cesium.Matrix4()); } var emitterModelMatrix = new Cesium.Matrix4(); var translation = new Cesium.Cartesian3(); var rotation = new Cesium.Quaternion(); var hpr = new Cesium.HeadingPitchRoll(); var trs = new Cesium.TranslationRotationScale(); function computeEmitterModelMatrix() { hpr = Cesium.HeadingPitchRoll.fromDegrees(0.0, 0.0, 0.0, hpr); trs.translation = Cesium.Cartesian3.fromElements(-4.0, 0.0, 1.4, translation); trs.rotation = Cesium.Quaternion.fromHeadingPitchRoll(hpr, rotation); return Cesium.Matrix4.fromTranslationRotationScale(trs, emitterModelMatrix); } var pos1 = Cesium.Cartesian3.fromDegrees(-75.15787310614596, 39.97862668312678); var pos2 = Cesium.Cartesian3.fromDegrees(-75.1633691390455, 39.95355089912078); var position = new Cesium.SampledPositionProperty(); position.addSample(start, pos1); position.addSample(stop, pos2); var entity = viewer.entities.add({ availability : new Cesium.TimeIntervalCollection([new Cesium.TimeInterval({ start : start, stop : stop })]), model : { uri : '../SampleData/models/CesiumMilkTruck/CesiumMilkTruck-kmc.glb', minimumPixelSize : 64 }, viewFrom: new Cesium.Cartesian3(-100.0, 0.0, 100.0), position : position, orientation : new Cesium.VelocityOrientationProperty(position) }); viewer.trackedEntity = entity; var scene = viewer.scene; var particleSystem = scene.primitives.add(new Cesium.ParticleSystem({ image : '../SampleData/smoke.png', startColor : Cesium.Color.LIGHTSEAGREEN.withAlpha(0.7), endColor : Cesium.Color.WHITE.withAlpha(0.0), startScale : viewModel.startScale, endScale : viewModel.endScale, minimumParticleLife : viewModel.minimumParticleLife, maximumParticleLife : viewModel.maximumParticleLife, minimumSpeed : viewModel.minimumSpeed, maximumSpeed : viewModel.maximumSpeed, imageSize : new Cesium.Cartesian2(viewModel.particleSize, viewModel.particleSize), emissionRate : viewModel.emissionRate, bursts : [ // these burst will occasionally sync to create a multicolored effect new Cesium.ParticleBurst({time : 5.0, minimum : 10, maximum : 100}), new Cesium.ParticleBurst({time : 10.0, minimum : 50, maximum : 100}), new Cesium.ParticleBurst({time : 15.0, minimum : 200, maximum : 300}) ], lifetime : 16.0, emitter : new Cesium.CircleEmitter(2.0), emitterModelMatrix : computeEmitterModelMatrix(), updateCallback : applyGravity })); var gravityScratch = new Cesium.Cartesian3(); function applyGravity(p, dt) { // We need to compute a local up vector for each particle in geocentric space. var position = p.position; Cesium.Cartesian3.normalize(position, gravityScratch); Cesium.Cartesian3.multiplyByScalar(gravityScratch, viewModel.gravity * dt, gravityScratch); p.velocity = Cesium.Cartesian3.add(p.velocity, gravityScratch, p.velocity); } viewer.scene.preUpdate.addEventListener(function(scene, time) { particleSystem.modelMatrix = computeModelMatrix(entity, time); // Account for any changes to the emitter model matrix. particleSystem.emitterModelMatrix = computeEmitterModelMatrix(); // Spin the emitter if enabled. if (viewModel.spin) { viewModel.heading += 1.0; viewModel.pitch += 1.0; viewModel.roll += 1.0; } }); Cesium.knockout.getObservable(viewModel, 'emissionRate').subscribe( function(newValue) { particleSystem.emissionRate = parseFloat(newValue); } ); Cesium.knockout.getObservable(viewModel, 'particleSize').subscribe( function(newValue) { var particleSize = parseFloat(newValue); particleSystem.minimumImageSize.x = particleSize; particleSystem.minimumImageSize.y = particleSize; particleSystem.maximumImageSize.x = particleSize; particleSystem.maximumImageSize.y = particleSize; } ); Cesium.knockout.getObservable(viewModel, 'minimumParticleLife').subscribe( function(newValue) { particleSystem.minimumParticleLife = parseFloat(newValue); } ); Cesium.knockout.getObservable(viewModel, 'maximumParticleLife').subscribe( function(newValue) { particleSystem.maximumParticleLife = parseFloat(newValue); } ); Cesium.knockout.getObservable(viewModel, 'minimumSpeed').subscribe( function(newValue) { particleSystem.minimumSpeed = parseFloat(newValue); } ); Cesium.knockout.getObservable(viewModel, 'maximumSpeed').subscribe( function(newValue) { particleSystem.maximumSpeed = parseFloat(newValue); } ); Cesium.knockout.getObservable(viewModel, 'startScale').subscribe( function(newValue) { particleSystem.startScale = parseFloat(newValue); } ); Cesium.knockout.getObservable(viewModel, 'endScale').subscribe( function(newValue) { particleSystem.endScale = parseFloat(newValue); } ); var options = [{ text : 'Circle Emitter', onselect : function() { particleSystem.emitter = new Cesium.CircleEmitter(2.0); } }, { text : 'Sphere Emitter', onselect : function() { particleSystem.emitter = new Cesium.SphereEmitter(2.5); } }, { text : 'Cone Emitter', onselect : function() { particleSystem.emitter = new Cesium.ConeEmitter(Cesium.Math.toRadians(45.0)); } }, { text : 'Box Emitter', onselect : function() { particleSystem.emitter = new Cesium.BoxEmitter(new Cesium.Cartesian3(10.0, 10.0, 10.0)); } }]; Sandcastle.addToolbarMenu(options); &lt;style&gt; @import url(../templates/bucket.css); #toolbar { background: rgba(42, 42, 42, 0.8); padding: 4px; border-radius: 4px; } #toolbar input { vertical-align: middle; padding-top: 2px; padding-bottom: 2px; } #toolbar .header { font-weight: bold; } &lt;/style&gt; &lt;div id=&quot;cesiumContainer&quot; class=&quot;fullSize&quot;&gt;&lt;/div&gt; &lt;div id=&quot;loadingOverlay&quot;&gt;&lt;h1&gt;Loading...&lt;/h1&gt;&lt;/div&gt; &lt;div id=&quot;toolbar&quot;&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Rate&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.0&quot; max=&quot;100.0&quot; step=&quot;1&quot; data-bind=&quot;value: emissionRate, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: emissionRate&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Size&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;2&quot; max=&quot;60.0&quot; step=&quot;1&quot; data-bind=&quot;value: particleSize, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: particleSize&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Min Life&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.1&quot; max=&quot;30.0&quot; step=&quot;1&quot; data-bind=&quot;value: minimumParticleLife, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: minimumParticleLife&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Max Life&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.1&quot; max=&quot;30.0&quot; step=&quot;1&quot; data-bind=&quot;value: maximumParticleLife, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: maximumParticleLife&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Min Speed&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.0&quot; max=&quot;30.0&quot; step=&quot;1&quot; data-bind=&quot;value: minimumSpeed, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: minimumSpeed&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Max Speed&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.0&quot; max=&quot;30.0&quot; step=&quot;1&quot; data-bind=&quot;value: maximumSpeed, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: maximumSpeed&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Start Scale&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.0&quot; max=&quot;10.0&quot; step=&quot;1&quot; data-bind=&quot;value: startScale, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: startScale&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;End Scale&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0.0&quot; max=&quot;10.0&quot; step=&quot;1&quot; data-bind=&quot;value: endScale, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: endScale&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gravity&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;-20.0&quot; max=&quot;20.0&quot; step=&quot;1&quot; data-bind=&quot;value: gravity, valueUpdate: 'input'&quot;&gt; &lt;input type=&quot;text&quot; size=&quot;5&quot; data-bind=&quot;value: gravity&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; ","link":"https://ibiji.top/post/1597298774980907/"},{"title":"cesium(笔记-3)","content":"运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro Cesium支持流式的、可视化的全球高程投影地形地势、水形数据，包括海洋、湖泊、河流、山峰、峡谷和其他能够被三维展示出来的且效果比二维好的地形数据。像图层数据一样，Cesium引擎会从一个服务器上请求流式地形数据，仅请求那些基于当前相机能看到的需要绘制的图层上的数据。 // Load Cesium World Terrain viewer.terrainProvider = Cesium.createWorldTerrain({ requestWaterMask : true, // required for water effects requestVertexNormals : true // required for terrain lighting }); // Enable depth testing so things behind the terrain disappear. viewer.scene.globe.depthTestAgainstTerrain = true; 高投影的arctic terrain // High-resolution arctic terrain from the Arctic DEM project (Release 4), tiled and hosted by Cesium ion. // https://www.pgc.umn.edu/data/arcticdem/ var viewer = new Cesium.Viewer('cesiumContainer', { terrainProvider: new Cesium.CesiumTerrainProvider({ url: Cesium.IonResource.fromAssetId(3956) }) }); // Add Alaskan locations Sandcastle.addDefaultToolbarMenu([{ text: 'Denali', onselect: function() { viewer.scene.camera.flyTo({ destination: Cesium.Cartesian3.fromRadians(-2.6399828792482234, 1.0993550795541742, 5795), orientation: { heading: 3.8455, pitch: -0.4535, roll: 0.0 } }); } }, { text: 'Anchorage Area', onselect: function() { viewer.scene.camera.flyTo({ destination: Cesium.Cartesian3.fromRadians(-2.610708034601548, 1.0671172431736584, 1900), orientation: { heading: 4.6, pitch: -0.341, roll: 0.0 } }); } }, { text: 'Peaks', onselect: function() { viewer.scene.camera.flyTo({ destination: Cesium.Cartesian3.fromRadians(-2.6928866820212813, 1.072394255273859, 3700), orientation: { heading: 1.6308222948889464, pitch: -0.6473480165020193, roll: 0.0 } }); } }, { text: 'Riverbed', onselect: function() { viewer.scene.camera.flyTo({ destination: Cesium.Cartesian3.fromRadians(-2.6395623497608596, 1.0976443174490356, 2070), orientation: { heading: 6.068794108659519, pitch: -0.08514161789475816, roll: 0.0 } }); } }], 'toolbar'); 高投影的Pennsylvania terrain // High resolution terrain of Pennsylvania curated by Pennsylvania Spatial Data Access (PASDA) // http://www.pasda.psu.edu/ var viewer = new Cesium.Viewer('cesiumContainer', { terrainProvider: new Cesium.CesiumTerrainProvider({ url: Cesium.IonResource.fromAssetId(3957) }) }); // Add PA locations Sandcastle.addDefaultToolbarMenu([{ text : 'Pinnacle', onselect : function() { viewer.scene.camera.flyTo({ destination : Cesium.Cartesian3.fromRadians(-1.3324415110874286, 0.6954224325279967, 236.6770689945084), orientation : { heading : Cesium.Math.toRadians(310), pitch : Cesium.Math.toRadians(-15), roll : 0.0 } }); } }, { text : 'Mount Nittany', onselect : function() { viewer.scene.camera.flyTo({ destination : Cesium.Cartesian3.fromRadians(-1.358985133937573, 0.7123252393978314, 451.05748252867375), orientation : { heading : Cesium.Math.toRadians(85), pitch : Cesium.Math.toRadians(0), roll : 0.0 } }); } }, { text : 'Horseshoe Curve', onselect : function() { viewer.scene.camera.flyTo({ destination : Cesium.Cartesian3.fromRadians(-1.3700147546199826, 0.706808606166025, 993.7916313325215), orientation : { heading : Cesium.Math.toRadians(90), pitch : Cesium.Math.toRadians(-15), roll : 0.0 } }); } }, { text : 'Jim Thorpe', onselect : function() { viewer.scene.camera.flyTo({ destination : Cesium.Cartesian3.fromRadians(-1.3218297501066052, 0.713358272291525, 240.87968743408845), orientation : { heading : Cesium.Math.toRadians(200), pitch : Cesium.Math.toRadians(-5), roll : 0.0 } }); } }, { text : 'Grand Canyon of PA', onselect : function() { viewer.scene.camera.flyTo({ destination : Cesium.Cartesian3.fromRadians(-1.349379633251472, 0.720297672225785, 656.268309953562), orientation : { heading : Cesium.Math.toRadians(200), pitch : Cesium.Math.toRadians(-5), roll : 0.0 } }); } }], 'toolbar'); ","link":"https://ibiji.top/post/1597298726342254/"},{"title":"cesium(笔记-2)","content":"运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro Cesium应用程序另一个关键元素是Imagery(图层)。瓦片图集合根据不同的投影方式映射到虚拟的三维数字地球表面。依赖于相机指向地表的方向和距离，Cesium会去请求和渲染不同层级的图层详细信息。 多种图层能够被添加、移除、排序和适应到Cesium中。 Cesium提供了一系列方法用于处理图层，比如颜色自适应，图层叠加融合 支持的图层格式： 1.wms 2.TMS 3.WMTS (with time dynamic imagery) 4.ArcGIS 5.Bing Maps 6.Google Earth 7.Mapbox 8.OpenStreetMap 基本图层 var viewer = new Cesium.Viewer('cesiumContainer', { imageryProvider : Cesium.createWorldImagery({ style : Cesium.IonWorldImageryStyle.AERIAL_WITH_LABELS }), baseLayerPicker : false }); var layers = viewer.scene.imageryLayers; var blackMarble = layers.addImageryProvider(new Cesium.IonImageryProvider({ assetId: 3812 })); blackMarble.alpha = 0.5; blackMarble.brightness = 2.0; layers.addImageryProvider(new Cesium.SingleTileImageryProvider({ url : '../images/Cesium_Logo_overlay.png', rectangle : Cesium.Rectangle.fromDegrees(-75.0, 28.0, -67.0, 29.75) })); 自适应图层 var viewer = new Cesium.Viewer('cesiumContainer'); var imageryLayers = viewer.imageryLayers; // The viewModel tracks the state of our mini application. var viewModel = { brightness: 0, contrast: 0, hue: 0, saturation: 0, gamma: 0 }; // Convert the viewModel members into knockout observables. Cesium.knockout.track(viewModel); // Bind the viewModel to the DOM elements of the UI that call for it. var toolbar = document.getElementById('toolbar'); Cesium.knockout.applyBindings(viewModel, toolbar); // Make the active imagery layer a subscriber of the viewModel. function subscribeLayerParameter(name) { Cesium.knockout.getObservable(viewModel, name).subscribe( function(newValue) { if (imageryLayers.length &gt; 0) { var layer = imageryLayers.get(0); layer[name] = newValue; } } ); } subscribeLayerParameter('brightness'); subscribeLayerParameter('contrast'); subscribeLayerParameter('hue'); subscribeLayerParameter('saturation'); subscribeLayerParameter('gamma'); // Make the viewModel react to base layer changes. function updateViewModel() { if (imageryLayers.length &gt; 0) { var layer = imageryLayers.get(0); viewModel.brightness = layer.brightness; viewModel.contrast = layer.contrast; viewModel.hue = layer.hue; viewModel.saturation = layer.saturation; viewModel.gamma = layer.gamma; } } imageryLayers.layerAdded.addEventListener(updateViewModel); imageryLayers.layerRemoved.addEventListener(updateViewModel); imageryLayers.layerMoved.addEventListener(updateViewModel); updateViewModel(); 控制调整图层顺序 var viewer = new Cesium.Viewer('cesiumContainer', { baseLayerPicker : false }); var imageryLayers = viewer.imageryLayers; var viewModel = { layers : [], baseLayers : [], upLayer : null, downLayer : null, selectedLayer : null, isSelectableLayer : function(layer) { return this.baseLayers.indexOf(layer) &gt;= 0; }, raise : function(layer, index) { imageryLayers.raise(layer); viewModel.upLayer = layer; viewModel.downLayer = viewModel.layers[Math.max(0, index - 1)]; updateLayerList(); window.setTimeout(function() { viewModel.upLayer = viewModel.downLayer = null; }, 10); }, lower : function(layer, index) { imageryLayers.lower(layer); viewModel.upLayer = viewModel.layers[Math.min(viewModel.layers.length - 1, index + 1)]; viewModel.downLayer = layer; updateLayerList(); window.setTimeout(function() { viewModel.upLayer = viewModel.downLayer = null; }, 10); }, canRaise : function(layerIndex) { return layerIndex &gt; 0; }, canLower : function(layerIndex) { return layerIndex &gt;= 0 &amp;&amp; layerIndex &lt; imageryLayers.length - 1; } }; var baseLayers = viewModel.baseLayers; Cesium.knockout.track(viewModel); function setupLayers() { // Create all the base layers that this example will support. // These base layers aren't really special. It's possible to have multiple of them // enabled at once, just like the other layers, but it doesn't make much sense because // all of these layers cover the entire globe and are opaque. addBaseLayerOption( 'Bing Maps Aerial', undefined); // the current base layer addBaseLayerOption( 'Bing Maps Road', new Cesium.BingMapsImageryProvider({ url : 'https://dev.virtualearth.net', mapStyle : Cesium.BingMapsStyle.ROAD })); addBaseLayerOption( 'ArcGIS World Street Maps', new Cesium.ArcGisMapServerImageryProvider({ url : 'https://services.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer' })); addBaseLayerOption( 'OpenStreetMaps', new Cesium.OpenStreetMapImageryProvider()); addBaseLayerOption( 'Stamen Maps', new Cesium.OpenStreetMapImageryProvider({ url : 'https://stamen-tiles.a.ssl.fastly.net/watercolor/', fileExtension : 'jpg', credit : 'Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under CC BY SA.' })); addBaseLayerOption( 'Natural Earth II (local)', new Cesium.TileMapServiceImageryProvider({ url : Cesium.buildModuleUrl('Assets/Textures/NaturalEarthII') })); addBaseLayerOption( 'USGS Shaded Relief (via WMTS)', new Cesium.WebMapTileServiceImageryProvider({ url : 'http://basemap.nationalmap.gov/arcgis/rest/services/USGSShadedReliefOnly/MapServer/WMTS', layer : 'USGSShadedReliefOnly', style : 'default', format : 'image/jpeg', tileMatrixSetID : 'default028mm', maximumLevel : 19, credit : 'U. S. Geological Survey' })); // Create the additional layers addAdditionalLayerOption( 'United States GOES Infrared', new Cesium.WebMapServiceImageryProvider({ url : 'https://mesonet.agron.iastate.edu/cgi-bin/wms/goes/conus_ir.cgi?', layers : 'goes_conus_ir', credit : 'Infrared data courtesy Iowa Environmental Mesonet', parameters : { transparent : 'true', format : 'image/png' } })); addAdditionalLayerOption( 'United States Weather Radar', new Cesium.WebMapServiceImageryProvider({ url : 'https://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi?', layers : 'nexrad-n0r', credit : 'Radar data courtesy Iowa Environmental Mesonet', parameters : { transparent : 'true', format : 'image/png' } })); addAdditionalLayerOption( 'TileMapService Image', new Cesium.TileMapServiceImageryProvider({ url : '../images/cesium_maptiler/Cesium_Logo_Color' }), 0.2); addAdditionalLayerOption( 'Single Image', new Cesium.SingleTileImageryProvider({ url : '../images/Cesium_Logo_overlay.png', rectangle : Cesium.Rectangle.fromDegrees(-115.0, 38.0, -107, 39.75) }), 1.0); addAdditionalLayerOption( 'Grid', new Cesium.GridImageryProvider(), 1.0, false); addAdditionalLayerOption( 'Tile Coordinates', new Cesium.TileCoordinatesImageryProvider(), 1.0, false); } function addBaseLayerOption(name, imageryProvider) { var layer; if (typeof imageryProvider === 'undefined') { layer = imageryLayers.get(0); viewModel.selectedLayer = layer; } else { layer = new Cesium.ImageryLayer(imageryProvider); } layer.name = name; baseLayers.push(layer); } function addAdditionalLayerOption(name, imageryProvider, alpha, show) { var layer = imageryLayers.addImageryProvider(imageryProvider); layer.alpha = Cesium.defaultValue(alpha, 0.5); layer.show = Cesium.defaultValue(show, true); layer.name = name; Cesium.knockout.track(layer, ['alpha', 'show', 'name']); } function updateLayerList() { var numLayers = imageryLayers.length; viewModel.layers.splice(0, viewModel.layers.length); for (var i = numLayers - 1; i &gt;= 0; --i) { viewModel.layers.push(imageryLayers.get(i)); } } setupLayers(); updateLayerList(); //Bind the viewModel to the DOM elements of the UI that call for it. var toolbar = document.getElementById('toolbar'); Cesium.knockout.applyBindings(viewModel, toolbar); Cesium.knockout.getObservable(viewModel, 'selectedLayer').subscribe(function(baseLayer) { // Handle changes to the drop-down base layer selector. var activeLayerIndex = 0; var numLayers = viewModel.layers.length; for (var i = 0; i &lt; numLayers; ++i) { if (viewModel.isSelectableLayer(viewModel.layers[i])) { activeLayerIndex = i; break; } } var activeLayer = viewModel.layers[activeLayerIndex]; var show = activeLayer.show; var alpha = activeLayer.alpha; imageryLayers.remove(activeLayer, false); imageryLayers.add(baseLayer, numLayers - activeLayerIndex - 1); baseLayer.show = show; baseLayer.alpha = alpha; updateLayerList(); }); &lt;style&gt; @import url(../templates/bucket.css); #toolbar { background: rgba(42, 42, 42, 0.8); padding: 4px; border-radius: 4px; } #toolbar input { vertical-align: middle; padding-top: 2px; padding-bottom: 2px; } #toolbar table tr { transform: translateY(0); transition: transform 0.4s ease-out; } #toolbar table tr.up { transform: translateY(33px); transition: none; } #toolbar table tr.down { transform: translateY(-33px); transition: none; } &lt;/style&gt; &lt;div id=&quot;cesiumContainer&quot; class=&quot;fullSize&quot;&gt;&lt;/div&gt; &lt;div id=&quot;loadingOverlay&quot;&gt;&lt;h1&gt;Loading...&lt;/h1&gt;&lt;/div&gt; &lt;div id=&quot;toolbar&quot;&gt; &lt;table&gt; &lt;tbody data-bind=&quot;foreach: layers&quot;&gt; &lt;tr data-bind=&quot;css: { up: $parent.upLayer === $data, down: $parent.downLayer === $data }&quot;&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; data-bind=&quot;checked: show&quot;&gt;&lt;/td&gt; &lt;td&gt; &lt;span data-bind=&quot;text: name, visible: !$parent.isSelectableLayer($data)&quot;&gt;&lt;/span&gt; &lt;select data-bind=&quot;visible: $parent.isSelectableLayer($data), options: $parent.baseLayers, optionsText: 'name', value: $parent.selectedLayer&quot;&gt;&lt;/select&gt; &lt;/td&gt; &lt;td&gt; &lt;input type=&quot;range&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.01&quot; data-bind=&quot;value: alpha, valueUpdate: 'input'&quot;&gt; &lt;/td&gt; &lt;td&gt; &lt;button type=&quot;button&quot; class=&quot;cesium-button&quot; data-bind=&quot;click: function() { $parent.raise($data, $index()); }, visible: $parent.canRaise($index())&quot;&gt; ▲ &lt;/button&gt; &lt;/td&gt; &lt;td&gt; &lt;button type=&quot;button&quot; class=&quot;cesium-button&quot; data-bind=&quot;click: function() { $parent.lower($data, $index()); }, visible: $parent.canLower($index())&quot;&gt; ▼ &lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; 切割图层(卷帘) var viewer = new Cesium.Viewer('cesiumContainer', { imageryProvider : new Cesium.ArcGisMapServerImageryProvider({ url : 'https://services.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer' }), baseLayerPicker : false, infoBox : false }); var layers = viewer.imageryLayers; var earthAtNight = layers.addImageryProvider(new Cesium.IonImageryProvider({ assetId: 3812 })); earthAtNight.splitDirection = Cesium.ImagerySplitDirection.LEFT; // Only show to the left of the slider. // Sync the position of the slider with the split position var slider = document.getElementById('slider'); viewer.scene.imagerySplitPosition = (slider.offsetLeft) / slider.parentElement.offsetWidth; var handler = new Cesium.ScreenSpaceEventHandler(slider); var moveActive = false; function move(movement) { if(!moveActive) { return; } var relativeOffset = movement.endPosition.x ; var splitPosition = (slider.offsetLeft + relativeOffset) / slider.parentElement.offsetWidth; slider.style.left = 100.0 * splitPosition + '%'; viewer.scene.imagerySplitPosition = splitPosition; } handler.setInputAction(function() { moveActive = true; }, Cesium.ScreenSpaceEventType.LEFT_DOWN); handler.setInputAction(function() { moveActive = true; }, Cesium.ScreenSpaceEventType.PINCH_START); handler.setInputAction(move, Cesium.ScreenSpaceEventType.MOUSE_MOVE); handler.setInputAction(move, Cesium.ScreenSpaceEventType.PINCH_MOVE); handler.setInputAction(function() { moveActive = false; }, Cesium.ScreenSpaceEventType.LEFT_UP); handler.setInputAction(function() { moveActive = false; }, Cesium.ScreenSpaceEventType.PINCH_END); ","link":"https://ibiji.top/post/1597298686608552/"},{"title":"cesium(笔记-1)","content":"运行环境 | cesium 1.58 | webstorm 2018.2.2 | windows 10 pro 隐藏界面中的元素 var viewer = new Cesium.Viewer('cesiumContainer',{ geocoder:false, //查找位置工具，查找到之后会将镜头对准找到的地址，默认使用bing地图 homeButton:false, //视角返回初始位置 sceneModePicker:false, //选择视角的模式，有三种：3D，2D，哥伦布视图（CV） baseLayerPicker:false, //图层选择器，选择要显示的地图服务和地形服务. navigationHelpButton:false, //导航帮助按钮，显示默认的地图控制帮助. animation:false, //动画器件，控制视图动画的播放速度 creditContainer:&quot;credit&quot;, timeline:false, //时间线,指示当前时间，并允许用户跳到特定的时间 fullscreenButton:false, // vrButton:false, // }); 通过css控制 .cesium-viewer-toolbar, /* 右上角按钮组 */ .cesium-viewer-animationContainer, /* 左下角动画控件 */ .cesium-viewer-timelineContainer, /* 时间线 */ .cesium-viewer-bottom /* logo信息 */ { display: none; } .cesium-viewer-fullscreenContainer /* 全屏按钮 */ { position: absolute; top: -999em; } 显示帧速(FPS) viewer.scene.debugShowFramesPerSecond = true; ","link":"https://ibiji.top/post/1597298622682345/"},{"title":"kylin部署","content":"运行环境 | centos 7.0 |kylin-2.6.2-hbase1x Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 kylin 部署 下载 apache-kylin-2.6.2-bin-hbase1x.tar.gz 解压 tar -zxvf apache-kylin-2.6.2-bin-hbase1x.tar.gz -C /opt 添加环境变量 vi /etc/profile export hive_dependency=/opt/hive-1.2.2/conf:/opt/hive-1.2.2/lib*:/opt/hive-1.2.2/hcatalog/share/hcatalog/hive-hcatalog-core-2.0.0.jar 启动报错一 Something wrong with Hive CLI or Beeline, please execute Hive CLI or Beeline CLI in terminal to find the root cause. 修改 kylin/bin/find-hive-dependency.sh 部分代码 if [ &quot;${client_mode}&quot; == &quot;beeline&quot; ] then #beeline_shell=`$KYLIN_HOME/bin/get-properties.sh kylin.source.hive.beeline-shell` #beeline_params=`bash ${KYLIN_HOME}/bin/get-properties.sh kylin.source.hive.beeline-params` #hive_env=`${beeline_shell} ${hive_conf_properties} ${beeline_params} --outputformat=dsv -e &quot;set;&quot; 2&gt;&amp;1 | grep --text 'env:CLASSPATH' ` beeline_params=`sh ${KYLIN_HOME}/bin/get-properties.sh kylin.hive.beeline.params` hive_env=`beeline ${beeline_params} --outputformat=dsv -e set | grep 'env:CLASSPATH'` else source ${dir}/check-hive-usability.sh hive_env=`hive -e set | grep 'env:CLASSPATH'` #hive_env=`hive ${hive_conf_properties} -e set 2&gt;&amp;1 | grep 'env:CLASSPATH'` #hive -e set &gt;/tmp/hive_env.txt 2&gt;&amp;1 #hive_env=`grep 'env:CLASSPATH' /tmp/hive_env.txt` #hive_env=`echo ${hive_env#*env:CLASSPATH}` #hive_env=&quot;env:CLASSPATH&quot;${hive_env} fi 修改 kylin/conf/kylin.properties kylin.rest.servers=192.168.83.128:7070,192.168.83.129:7070,192.168.83.130:7070 kylin.rest.timezone=GMT+8 kylin.hive.client=beeline kylin.hive.beeline.params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u 'jdbc:hive2://192.168.83.128:10000' 分别验证 bin/find-hive-dependency.sh Retrieving hive dependency... Connecting to jdbc:hive2://192.168.83.128:10000 Connected to: Apache Hive (version 1.2.2) Driver: Hive JDBC (version 1.2.2) Transaction isolation: TRANSACTION_REPEATABLE_READ 1,484 rows selected (0.89 seconds) Beeline version 1.2.2 by Apache Hive Closing: 0: jdbc:hive2://192.168.83.128:10000 bin/find-hbase-dependency.sh Retrieving hbase dependency... 启动报错二 java.net.UnknownHostException: master:2181: 未知的名称或服务 at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at org.apache.zookeeper.client.StaticHostProvider.&lt;init&gt;(StaticHostProvider.java:61) at org.apache.zookeeper.ZooKeeper.&lt;init&gt;(ZooKeeper.java:445) at org.apache.curator.utils.DefaultZookeeperFactory.newZooKeeper(DefaultZookeeperFactory.java:29) at org.apache.curator.framework.imps.CuratorFrameworkImpl$2.newZooKeeper(CuratorFrameworkImpl.java:154) at org.apache.curator.HandleHolder$1.getZooKeeper(HandleHolder.java:94) at org.apache.curator.HandleHolder.getZooKeeper(HandleHolder.java:55) at org.apache.curator.ConnectionState.reset(ConnectionState.java:219) at org.apache.curator.ConnectionState.start(ConnectionState.java:103) at org.apache.curator.CuratorZookeeperClient.start(CuratorZookeeperClient.java:190) at org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:256) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock$Factory.getZKClient(ZookeeperDistributedLock.java:85) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock$Factory.&lt;init&gt;(ZookeeperDistributedLock.java:109) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock$Factory.&lt;init&gt;(ZookeeperDistributedLock.java:105) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.util.ClassUtil.newInstance(ClassUtil.java:88) at org.apache.kylin.common.KylinConfigBase.getDistributedLockFactory(KylinConfigBase.java:458) at org.apache.kylin.storage.hbase.HBaseConnection.createHTableIfNeeded(HBaseConnection.java:327) at org.apache.kylin.storage.hbase.HBaseResourceStore.createHTableIfNeeded(HBaseResourceStore.java:114) at org.apache.kylin.storage.hbase.HBaseResourceStore.&lt;init&gt;(HBaseResourceStore.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:92) at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:111) at org.apache.kylin.rest.service.AclTableMigrationTool.checkIfNeedMigrate(AclTableMigrationTool.java:99) at org.apache.kylin.tool.AclTableMigrationCLI.main(AclTableMigrationCLI.java:43) 2019-05-20 20:11:14,464 DEBUG [main] util.ZookeeperDistributedLock:147 : 27720@master trying to lock /kylin/kylin_metadata/create_htable/kylin_metadata/lock 2019-05-20 20:11:29,653 ERROR [main] curator.ConnectionState:201 : Connection timed out for connection string (master:2181:2181,slave0:2181:2181,slave1:2181:2181) and timeout (15000) / elapsed (21695) org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:198) at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88) at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115) at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:474) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668) at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453) at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443) at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock.lock(ZookeeperDistributedLock.java:150) at org.apache.kylin.storage.hbase.util.ZookeeperDistributedLock.lock(ZookeeperDistributedLock.java:171) at org.apache.kylin.storage.hbase.HBaseConnection.createHTableIfNeeded(HBaseConnection.java:328) at org.apache.kylin.storage.hbase.HBaseResourceStore.createHTableIfNeeded(HBaseResourceStore.java:114) at org.apache.kylin.storage.hbase.HBaseResourceStore.&lt;init&gt;(HBaseResourceStore.java:88) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:92) at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:111) at org.apache.kylin.rest.service.AclTableMigrationTool.checkIfNeedMigrate(AclTableMigrationTool.java:99) at org.apache.kylin.tool.AclTableMigrationCLI.main(AclTableMigrationCLI.java:43) 修改 hbase-site.xml文件 &lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave0,slave1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.support.append&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;source&gt;2181默认值，来源hbase-default.xml&lt;/source&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/root/tmp/hbase/zookeeper&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; http://192.168.83.128:7070/kylin 2019-05-20 22:39:35,530 INFO [main] util.ZookeeperDistributedLock:238 : 70746@master released lock at /kylin/kylin_metadata/create_htable/kylin_metadata/lock Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Failed to find metadata store by url: kylin_metadata@hbase at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:99) at org.apache.kylin.common.persistence.ResourceStore.getStore(ResourceStore.java:111) at org.apache.kylin.rest.service.AclTableMigrationTool.checkIfNeedMigrate(AclTableMigrationTool.java:99) at org.apache.kylin.tool.AclTableMigrationCLI.main(AclTableMigrationCLI.java:43) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.kylin.common.persistence.ResourceStore.createResourceStore(ResourceStore.java:92) ... 3 more Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=1, exceptions: Mon May 20 22:39:35 CST 2019, RpcRetryingCaller{globalStartTime=1558363169520, pause=100, retries=1}, java.io.IOException: Call to master/192.168.83.128:16000 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=7, waitTime=5001, operationTimeout=5000 expired. at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:157) at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:4297) at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:4289) at org.apache.hadoop.hbase.client.HBaseAdmin.createTableAsyncV2(HBaseAdmin.java:753) at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:674) at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:607) at org.apache.kylin.storage.hbase.HBaseConnection.createHTableIfNeeded(HBaseConnection.java:347) at org.apache.kylin.storage.hbase.HBaseResourceStore.createHTableIfNeeded(HBaseResourceStore.java:114) at org.apache.kylin.storage.hbase.HBaseResourceStore.&lt;init&gt;(HBaseResourceStore.java:88) ... 8 more Caused by: java.io.IOException: Call to master/192.168.83.128:16000 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=7, waitTime=5001, operationTimeout=5000 expired. at org.apache.hadoop.hbase.ipc.AbstractRpcClient.wrapException(AbstractRpcClient.java:292) at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1276) at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:227) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.createTable(MasterProtos.java:58551) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$4.createTable(ConnectionManager.java:1864) at org.apache.hadoop.hbase.client.HBaseAdmin$5.call(HBaseAdmin.java:762) at org.apache.hadoop.hbase.client.HBaseAdmin$5.call(HBaseAdmin.java:754) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136) ... 16 more Caused by: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=7, waitTime=5001, operationTimeout=5000 expired. at org.apache.hadoop.hbase.ipc.Call.checkAndSetTimeout(Call.java:73) at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1250) ... 23 more 2019-05-20 22:39:35,682 INFO [close-hbase-conn] hbase.HBaseConnection:136 : Closing HBase connections... 2019-05-20 22:39:35,683 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:2155 : Closing master protocol: MasterService 2019-05-20 22:39:35,684 INFO [close-hbase-conn] client.ConnectionManager$HConnectionImplementation:1726 : Closing zookeeper sessionid=0x36ad53f63430007 2019-05-20 22:39:35,749 INFO [main-EventThread] zookeeper.ClientCnxn:512 : EventThread shut down 2019-05-20 22:39:35,749 INFO [close-hbase-conn] zookeeper.ZooKeeper:684 : Session: 0x36ad53f63430007 closed 2019-05-20 22:39:36,113 INFO [Thread-6] zookeeper.ZooKeeper:684 : Session: 0x36ad53f63430008 closed 2019-05-20 22:39:36,114 INFO [main-EventThread] zookeeper.ClientCnxn:512 : EventThread shut down ERROR: Unknown error. Please check full log. 提取出重要的内容 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hbase.TableExistsException): kylin_metadata Caused by: org.apache.hadoop.hbase.TableExistsException: kylin_metadata 进入 zookeeper zkCli.sh /opt/zookeeper-3.4.6/bin/zkCli.sh ls /hbase/table rmr /hbase/table/kylin_metadata 重启hbase 登陆 http://host:7070/kylin 没有反应 注释 kylin/tomcat/conf/server.xml &lt;Connector port=&quot;7443&quot; protocol=&quot;org.apache.coyote.http11.Http11Protocol&quot; maxThreads=&quot;150&quot; SSLEnabled=&quot;true&quot; scheme=&quot;https&quot; secure=&quot;true&quot; keystoreFile=&quot;conf/.keystore&quot; keystorePass=&quot;changeit&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; /&gt; 补充： hive 启动时无法访问spark-assembly-*.jar的解决办法 将加载原来的lib/spark-assembly-*.jar`替换成jars/*.jar 部分代码如下 # add Spark assembly jar to the classpath if [[ -n &quot;$SPARK_HOME&quot; ]] then #sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar` sparkAssemblyPath=`ls ${SPARK_HOME}/jars/*.jar` CLASSPATH=&quot;${CLASSPATH}:${sparkAssemblyPath}&quot; fi ","link":"https://ibiji.top/post/1597299619526989/"},{"title":"kafka生产者启动报错及kafka常用命令","content":"运行环境 | centos 6.6 | kafka_2.11-1.1.1 kafka启动生产者后报错 WARN [Producer clientId=console-producer] Got error produce response with correlation id 3 on topic-partition test-2, retrying (2 attempts left). Error: NETWORK_EXCEPTION (org.apache.kafka.clients.producer.internals.Sender) 运行生产者命令后报错 ./kafka-console-producer.sh --broker-list 192.168.83.128:9092 --topic test &gt;[2019-04-29 10:37:39,906] WARN [Producer clientId=console-producer] Got error produce response with correlation id 3 on topic-partition test-2, retrying (2 attempts left). Error: NETWORK_EXCEPTION (org.apache.kafka.clients.producer.internals.Sender) 运行消费者报错 ./kafka-console-consumer.sh -zookeeper 192.168.83.129:2181 --from-beginning --topic test Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. 查看日志 [2019-05-02 16:10:25,182] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer) kafka.common.InconsistentBrokerIdException: Configured broker.id 2 doesn't match stored broker.id 0 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs). at kafka.server.KafkaServer.getBrokerIdAndOfflineDirs(KafkaServer.scala:673) at kafka.server.KafkaServer.startup(KafkaServer.scala:209) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38) at kafka.Kafka$.main(Kafka.scala:75) at kafka.Kafka.main(Kafka.scala) [2019-05-02 16:10:25,185] INFO shutting down (kafka.server.KafkaServer) [2019-05-02 16:10:25,209] WARN (kafka.utils.CoreUtils$) java.lang.NullPointerException at kafka.server.KafkaServer$$anonfun$shutdown$5.apply$mcV$sp(KafkaServer.scala:572) at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:85) at kafka.server.KafkaServer.shutdown(KafkaServer.scala:572) at kafka.server.KafkaServer.startup(KafkaServer.scala:329) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38) at kafka.Kafka$.main(Kafka.scala:75) at kafka.Kafka.main(Kafka.scala) [2019-05-02 16:10:25,211] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient) [2019-05-02 16:10:25,244] INFO Session: 0x26a778905c40001 closed (org.apache.zookeeper.ZooKeeper) [2019-05-02 16:10:25,246] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient) [2019-05-02 16:10:25,250] INFO shut down completed (kafka.server.KafkaServer) [2019-05-02 16:10:25,250] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable) 原因： 所有配置 broker.id=0 修改 kafka-logs-data/meta.properties 修改后重启正常生产和消费 常用命令 生产者操作： ./kafka-console-producer.sh --broker-list master:9092 --topic test 消费者操作： ./kafka-console-consumer.sh --bootstrap-server master:9092 --topic test --from-beginning 创建topic ./kafka-topics.sh --create --zookeeper master:2181 --replication-factor 2 --partitions 3 --topic test 查看topic列表 ./kafka-topics.sh --list --zookeeper master:2181 如果需要查看topic的详细信息，需要使用describe命令 ./kafka-topics.sh --describe --zookeeper node1:2181 --topic test-topic 若不指定topic，则查看所有topic的信息 ./kafka-topics.sh --describe --zookeeper node1:2181 删除topic ./kafka-topics.sh --delete --zookeeper &lt;ZOOKEEPER IP:PORT&gt; --topic &lt;TOPIC NAME&gt; 修改topic副本分布生成 kafka-reassign-partitions.sh --zookeeper &lt;ZOOKEEPER IP:PORT&gt; --topics-to-move-json-file topic-generate.json --broker-list &quot;61,62,63&quot; --generate { &quot;topics&quot;:[{&quot;topic&quot;: &quot;&quot;}], &quot;version&quot;:1 } 修改topic副本分布生效 kafka-reassign-partitions --zookeeper &lt;ZOOKEEPER IP:PORT&gt; --reassignment-json-file partition-replica-reassignment.json --execute 修改topic同步带宽 kafka-reassign-partitions.sh --zookeeper &lt;ZOOKEEPER IP:PORT&gt; --execute --reassignment-json-file partition-replica-reassignment.json —throttle 50000000 更改kafka单个topic数据保存时间 kafka-topics.sh --zookeeper &lt;ZOOKEEPER IP:PORT&gt; -topic &lt;TOPIC NAME&gt; --alter --config retention.ms=259200000 执行删除 kafka-topics --zookeeper &lt;ZOOKEEPER IP:PORT&gt; --alter --topic pnode.billing --config cleanup.policy=delete topic 分区动态扩容 kafka-topics.sh --alter --zookeeper &lt;ZOOKEEPER IP:PORT&gt; --partitions &lt; partitions_NAME&gt; --topic &lt;TOPIC_NAME&gt; ","link":"https://ibiji.top/post/1597299561133532/"},{"title":"kafka压力、数据丢失测试","content":"运行环境 | centos 7.0 | kafka_2.11-1.1.1 | opt/zookeeper-3.4.6 | flume-1.8.0 | nginx-openresty/1.13.6.2 本文主要通过Nginx做日志服务器 以POST请求生成日志（每次请求约为275字节，JSON格式），利用lua脚本变为相应格式，flume指定Source为TailDirSouce、kafka Channel、配置ACK应答机制a1.channels.c1.kafka.producer.acks = -1失败重试次数a1.channels.c1.kafka.producer.retries = 3;kafka主题设定三个分区两个副本 主机名 备注 操作系统 运行服务 配置 master 控制节点 CentOs7 kafka、zookeeper、nginx、flume 1C 2GM slave0 计算节点 CentOs7 kafka、zookeeper 1C 1.5GM slave1 块存储节点 CentOs7 kafka、zookeeper 1C 1.5GM 注：C-核 M-内存 启动nginx /usr/local/openresty/nginx/sbin/nginx 启动zookeeper /opt/zookeeper-3.4.6/bin/zkServer.sh start 启动flume bin/flume-ng agent -n a1 -c conf/ -f myconf/nginx-kafka-kill.conf -Dflume.root.logger=INFO,consoledate 启动kafka /opt/kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon /opt/kafka_2.11-1.1.1/config/server.properties 以下为 IDEA 程序输出log数 与 Nginx采集到的log数、各个节点kafka消费到的log数对比 IDEA log Nginx log Master消费 slave0消费 slave1消费 1000 842 701 722 732 1500 1269 1041 1086 1111 2000 1727 1480 1493 1461 2500 2022 1665 1739 1655 3000 2315 1892 1983 2163 5000 4800 2924 2924 ---- 期间杀掉master节点kafka进程，slave0节点停止消费（这里很疑惑） 查看主题详情 bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic testkill Topic:testkill PartitionCount:3 ReplicationFactor:2 Configs: Topic: testkill Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1 Topic: testkill Partition: 1 Leader: 2 Replicas: 1,2 Isr: 2,1 Topic: testkill Partition: 2 Leader: 2 Replicas: 2,0 Isr: 2 IDEA程序中设定5000次请求，请求完后，启动kafka进程 恢复后查看节点详情 bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic testkill Topic:testkill PartitionCount:3 ReplicationFactor:2 Configs: Topic: testkill Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1,0 Topic: testkill Partition: 1 Leader: 2 Replicas: 1,2 Isr: 2,1 Topic: testkill Partition: 2 Leader: 2 Replicas: 2,0 Isr: 2,0 kafka启动后，消费之前没有消费的数据,进本地磁盘各分区查找比对数据，均在 结论：初步认定kafka高可用，数据0丢失（杀掉master节点kafka进程,slave0节点停止消费这里有待验证），吞吐量2000条一下压力正常，随着时间推移数据量越大积压越大 kafka offset 维护方式 zookeeper维护 使用zookeeper来维护offset kafka 0.9 以前的版本是将offset 存储在zookeeper上的，kafka在传输数据时，数据消费成功就会修改偏移量，这样就可以保证数据不会丢失而导致传输出错；但是这也存在一个问题：那就是每次消费数据时都要将数据的offset写入一次，效率比较低，而且zookeeper与kafka的offset变化确认也需要走网络IO，这样就会给offset的维护带来不稳定性和低效。 kafka自己维护offset 使用broker来维护offset kafka 0.9 以后，offset的使用了内部的roker来管理，这样仅仅只需要broker，而不要zookeeper来维护，都是将topic提交给__consumer_offsets函数来执行。 也可以将kafka偏移量存入redis中，利用redis事务性维护offset kafka消息的位置 含义 名称 earlieastLeaderOffsets 存储在broker上的leader节点的最早的消息偏移量 consumerOffsets 消费者消费的消息偏移量位置 情况一：正常情况下，消费的消息偏移量应该大于broker上存储的最早的消息偏移量，即 A &lt; B 情况二：如果A 依然小于 B，则仍可以正常消费 情况三：然而，当 A &gt; B 时，则说明还没有被消费的消息已经被清除 此种情况会抛出 kafka.common.OffsetOutOfRangeException 异常。 在没有外部系统清除kafka消息的情况下，协调设置broker的最大保留大小 log.retention.bytes 和 最大保留时间log.retention.hours 等，来配合消费者端的读取消息。可以通过读取和监控消费者消费的offsets，来保证消息不会被意外清除。 ","link":"https://ibiji.top/post/1597299591548714/"},{"title":"elasticsearch 启动报错(3)","content":"运行环境 | centos 7.0 | elasticsearch 6.5.4 elasticsearch 启动报错org.elasticsearch.discovery.MasterNotDiscoveredException: null [2019-04-12T22:25:44,586][WARN ][r.suppressed ] [master] path: /_cluster/health, params: {} org.elasticsearch.discovery.MasterNotDiscoveredException: null at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:246) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:317) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:244) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:559) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-6.5.4.jar:6.5.4] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131] [2019-04-12T22:25:44,594][DEBUG][o.e.a.a.c.h.TransportClusterHealthAction] [master] timed out while retrying [cluster:monitor/health] after failure (timeout [30s]) [2019-04-12T22:25:44,594][WARN ][r.suppressed ] [master] path: /_cluster/health, params: {} org.elasticsearch.discovery.MasterNotDiscoveredException: null at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:246) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:317) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:244) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:559) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-6.5.4.jar:6.5.4] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131] [2019-04-12T22:25:45,108][DEBUG][o.e.a.a.c.h.TransportClusterHealthAction] [master] timed out while retrying [cluster:monitor/health] after failure (timeout [30s]) [2019-04-12T22:25:45,108][WARN ][r.suppressed ] [master] path: /_cluster/health, params: {pretty=} org.elasticsearch.discovery.MasterNotDiscoveredException: null at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:246) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:317) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:244) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:559) [elasticsearch-6.5.4.jar:6.5.4] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-6.5.4.jar:6.5.4] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131] 原因：elasticsearch没有发现master和集群初始化主节点的配置为空 解决办法：修改elasticsearch.yml，添加如下配置 cluster.initial_master_nodes: [&quot;master&quot;] 可能是网络原因，下次遇到的话先检查网络是否畅通(仅供参考)： discovery.zen.fd.ping_timeout: 1m discovery.zen.fd.ping_retries: 5 ","link":"https://ibiji.top/post/1597299109989595/"},{"title":"elasticsearch 启动报错(2)","content":"运行环境 | centos 7.0 | elasticsearch 6.5.4 | VMware Workstation Pro 初次启动elasticsearch报错 [2019-04-11T17:32:11,932][INFO ][o.e.e.NodeEnvironment ] [uITtGcF] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [41.5gb], net total_space [49.9gb], types [rootfs] [doit@master ~]$ /opt/elasticsearch-6.5.4/bin/elasticsearch [2019-04-11T17:32:11,932][INFO ][o.e.e.NodeEnvironment ] [uITtGcF] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [41.5gb], net total_space [49.9gb], types [rootfs] [2019-04-11T17:32:11,990][INFO ][o.e.e.NodeEnvironment ] [uITtGcF] heap size [1015.6mb], compressed ordinary object pointers [true] [2019-04-11T17:32:11,993][INFO ][o.e.n.Node ] [uITtGcF] node name derived from node ID [uITtGcFxRLSFwkuE1FUtwg]; set [node.name] to override [2019-04-11T17:32:11,994][INFO ][o.e.n.Node ] [uITtGcF] version[6.5.4], pid[2833], build[default/tar/d2ef93d/2018-12-17T21:17:40.758843Z], OS[Linux/3.10.0-514.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_131/25.131-b11] [2019-04-11T17:32:11,994][INFO ][o.e.n.Node ] [uITtGcF] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch.nQEcpVMV, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/opt/elasticsearch-6.5.4, -Des.path.conf=/opt/elasticsearch-6.5.4/config, -Des.distribution.flavor=default, -Des.distribution.type=tar] [2019-04-11T17:32:29,133][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [aggs-matrix-stats] [2019-04-11T17:32:29,133][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [analysis-common] [2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [ingest-common] [2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [lang-expression] [2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [lang-mustache] [2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [lang-painless] [2019-04-11T17:32:29,134][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [mapper-extras] [2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [parent-join] [2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [percolator] [2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [rank-eval] [2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [reindex] [2019-04-11T17:32:29,135][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [repository-url] [2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [transport-netty4] [2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [tribe] [2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-ccr] [2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-core] [2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-deprecation] [2019-04-11T17:32:29,136][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-graph] [2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-logstash] [2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-ml] [2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-monitoring] [2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-rollup] [2019-04-11T17:32:29,137][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-security] [2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-sql] [2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-upgrade] [2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] loaded module [x-pack-watcher] [2019-04-11T17:32:29,138][INFO ][o.e.p.PluginsService ] [uITtGcF] no plugins loaded [2019-04-11T17:32:56,126][INFO ][o.e.x.s.a.s.FileRolesStore] [uITtGcF] parsed [0] roles from file [/opt/elasticsearch-6.5.4/config/roles.yml] [2019-04-11T17:33:00,018][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [uITtGcF] [controller/2892] [Main.cc@109] controller (64 bit): Version 6.5.4 (Build b616085ef32393) Copyright (c) 2018 Elasticsearch BV [2019-04-11T17:33:03,297][DEBUG][o.e.a.ActionModule ] [uITtGcF] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2019-04-11T17:33:05,848][INFO ][o.e.d.DiscoveryModule ] [uITtGcF] using discovery type [zen] and host providers [settings] [2019-04-11T17:33:09,911][INFO ][o.e.n.Node ] [uITtGcF] initialized [2019-04-11T17:33:09,912][INFO ][o.e.n.Node ] [uITtGcF] starting ... [2019-04-11T17:33:11,314][INFO ][o.e.t.TransportService ] [uITtGcF] publish_address {192.168.83.128:9300}, bound_addresses {192.168.83.128:9300} [2019-04-11T17:33:11,462][INFO ][o.e.b.BootstrapChecks ] [uITtGcF] bound or publishing to a non-loopback address, enforcing bootstrap checks ERROR: [1] bootstrap checks failed [1]: max number of threads [3782] for user [doit] is too low, increase to at least [4096] [2019-04-11T17:33:11,505][INFO ][o.e.n.Node ] [uITtGcF] stopping ... [2019-04-11T17:33:11,891][INFO ][o.e.n.Node ] [uITtGcF] stopped [2019-04-11T17:33:11,891][INFO ][o.e.n.Node ] [uITtGcF] closing ... [2019-04-11T17:33:11,957][INFO ][o.e.n.Node ] [uITtGcF] closed [2019-04-11T17:33:11,985][INFO ][o.e.x.m.j.p.NativeController] [uITtGcF] Native controller process has stopped - no new native processes can be started 问题原因 &amp; 解决方法 内存不足,加大内存,报错前内存是1G，修改为2G问题解决 ","link":"https://ibiji.top/post/1597299079524595/"},{"title":"elasticsearch 启动报错(1)","content":"运行环境 | centos 7.0 | elasticsearch 6.5.4 elasticsearch 启动报错 2019-04-11 21:24:31,724 main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class 2019-04-11 21:24:31,724 main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:235) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:135) at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:959) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:899) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:891) at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:514) at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:238) at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:250) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:547) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:263) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:234) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:127) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:302) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 2019-04-11 21:24:31,734 main ERROR RollingFileManager (/opt/elasticsearch-6.5.4/logs/mycluster_index_search_slowlog.log) java.io.FileNotFoundException: /opt/elasticsearch-6.5.4/logs/mycluster_index_search_slowlog.log (权限不够) java.io.FileNotFoundException: /opt/elasticsearch-6.5.4/logs/mycluster_index_search_slowlog.log (权限不够) at java.io.FileOutputStream.open0(Native Method) at java.io.FileOutputStream.open(FileOutputStream.java:270) at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213) at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:133) at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:640) at org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:608) at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:113) at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:114) at org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:188) at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:145) at org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:61) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:123) at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:959) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:899) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:891) at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:514) at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:238) at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:250) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:547) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:263) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:234) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:127) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:302) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 检查文件权限问题 系日志文件造成，删除执行命令的输出日志 以非root账号启动，启动成功 百度参考解决方法： elastic官网 红黑联盟 ","link":"https://ibiji.top/post/1597299043774517/"},{"title":"flume 启动agent报错","content":"运行环境 | ubuntu 18.4 | flume 1.8.0 flume 启动agent报错 Including Hadoop libraries found via (/opt/hadoop-2.8.5/bin/hadoop) for HDFS access Info: Including Hive libraries found via (/opt/hive-1.2.2) for Hive access exec /usr/jdk1.8/bin/java -Xmx20m -Dflume.root.logger=INFO,console antler@antler:/opt/flume-1.8.0$ ./bin/flume-ng agent -c conf/ -f myconf/flume.conf -Dflume.root.logger=INFO,consoleInfo: Including Hadoop libraries found via (/opt/hadoop-2.8.5/bin/hadoop) for HDFS access Info: Including Hive libraries found via (/opt/hive-1.2.2) for Hive access + exec /usr/jdk1.8/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp '/opt/flume-1.8.0/conf:/opt/flume-1.8.0/lib/*:/opt/hadoop-2.8.5/etc/hadoop:/opt/hadoop-2.8.5/share/hadoop/common/lib/*:/opt/hadoop-2.8.5/share/hadoop/common/*:/opt/hadoop-2.8.5/share/hadoop/hdfs:/opt/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/opt/hadoop-2.8.5/share/hadoop/hdfs/*:/opt/hadoop-2.8.5/share/hadoop/yarn/lib/*:/opt/hadoop-2.8.5/share/hadoop/yarn/*:/opt/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.8.5/share/hadoop/mapreduce/*:/opt/hadoop-2.8.5/contrib/capacity-scheduler/*.jar:/opt/hive-1.2.2/lib/*' -Djava.library.path=:/opt/hadoop-2.8.5/lib/native org.apache.flume.node.Application -f myconf/flume.conf SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/opt/flume-1.8.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 2019-04-06 19:58:40,247 (main) [ERROR - org.apache.flume.node.Application.main(Application.java:348)] A fatal error occurred while running. Exception follows. org.apache.commons.cli.MissingOptionException: Missing required option: n at org.apache.commons.cli.Parser.checkRequiredOptions(Parser.java:299) at org.apache.commons.cli.Parser.parse(Parser.java:231) at org.apache.commons.cli.Parser.parse(Parser.java:85) at org.apache.flume.node.Application.main(Application.java:263) 问题原因 &amp; 解决方法 命令错误 正确命令： bin/flume-ng agent -n a1 -c conf/ -f myconf/tail-avro.conf -Dflume.root.logger=INFO,console ","link":"https://ibiji.top/post/1597299151805585/"},{"title":"spark-sql启动报错util.NativeCodeLoader","content":"运行环境 | centos 7.0 | spark 2.2.0 | scala-2.11.8 | mysql-connector-java-5.1.38.jar 启动saprk-sql报错 19/03/16 20:40:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable [root@master bin]# ./spark-sql --master spark://master:7077 --jars /root/mysql-connector-java-5.1.38.jar 19/03/16 20:40:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 19/03/16 20:40:48 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore 19/03/16 20:40:48 INFO metastore.ObjectStore: ObjectStore, initialize called 19/03/16 20:40:49 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored 19/03/16 20:40:49 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored 19/03/16 20:40:58 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot; 19/03/16 20:41:03 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table. 19/03/16 20:41:03 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table. 19/03/16 20:41:05 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table. 19/03/16 20:41:05 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table. 19/03/16 20:41:06 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY 19/03/16 20:41:06 INFO metastore.ObjectStore: Initialized ObjectStore 19/03/16 20:41:07 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0 19/03/16 20:41:07 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException 19/03/16 20:41:11 INFO metastore.HiveMetaStore: Added admin role in metastore 19/03/16 20:41:11 INFO metastore.HiveMetaStore: Added public role in metastore 19/03/16 20:41:11 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty 19/03/16 20:41:12 INFO metastore.HiveMetaStore: 0: get_all_databases 19/03/16 20:41:12 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_all_databases 19/03/16 20:41:12 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=* 19/03/16 20:41:12 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_functions: db=default pat=* 19/03/16 20:41:12 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table. 19/03/16 20:41:13 INFO session.SessionState: Created local directory: /tmp/6a909914-1d3e-40f9-b997-ec14d17ef1db_resources 19/03/16 20:41:14 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/6a909914-1d3e-40f9-b997-ec14d17ef1db 19/03/16 20:41:14 INFO session.SessionState: Created local directory: /tmp/root/6a909914-1d3e-40f9-b997-ec14d17ef1db 19/03/16 20:41:14 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/6a909914-1d3e-40f9-b997-ec14d17ef1db/_tmp_space.db 19/03/16 20:41:15 INFO spark.SparkContext: Running Spark version 2.2.0 19/03/16 20:41:15 INFO spark.SparkContext: Submitted application: SparkSQL::192.168.83.10 19/03/16 20:41:15 INFO spark.SecurityManager: Changing view acls to: root 19/03/16 20:41:15 INFO spark.SecurityManager: Changing modify acls to: root 19/03/16 20:41:15 INFO spark.SecurityManager: Changing view acls groups to: 19/03/16 20:41:15 INFO spark.SecurityManager: Changing modify acls groups to: 19/03/16 20:41:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set() 19/03/16 20:41:17 INFO util.Utils: Successfully started service 'sparkDriver' on port 58142. 19/03/16 20:41:17 INFO spark.SparkEnv: Registering MapOutputTracker 19/03/16 20:41:17 INFO spark.SparkEnv: Registering BlockManagerMaster 19/03/16 20:41:17 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information 19/03/16 20:41:17 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up 19/03/16 20:41:17 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f02e18ee-fc40-4a05-80ba-0c60c369ed5a 19/03/16 20:41:17 INFO memory.MemoryStore: MemoryStore started with capacity 413.9 MB 19/03/16 20:41:18 INFO spark.SparkEnv: Registering OutputCommitCoordinator 19/03/16 20:41:18 INFO util.log: Logging initialized @42429ms 19/03/16 20:41:19 INFO server.Server: jetty-9.3.z-SNAPSHOT 19/03/16 20:41:19 INFO server.Server: Started @42899ms 19/03/16 20:41:19 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041. 19/03/16 20:41:19 INFO server.AbstractConnector: Started ServerConnector@2e49b91c{HTTP/1.1,[http/1.1]}{0.0.0.0:4041} 19/03/16 20:41:19 INFO util.Utils: Successfully started service 'SparkUI' on port 4041. 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c7f6b5{/jobs,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53bb71e5{/jobs/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15994b0b{/jobs/job,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cb00fa5{/jobs/job/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698d6d30{/stages,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3407aa4f{/stages/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@538b3c88{/stages/stage,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22ae905f{/stages/stage/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fbaa7f5{/stages/pool,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d4a05f7{/stages/pool/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b476233{/storage,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f235e8e{/storage/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29dcdd1c{/storage/rdd,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@524f5ea5{/storage/rdd/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17134190{/environment,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5599b5bb{/environment/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4264beb8{/executors,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7cd3e0da{/executors/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67e77f52{/executors/threadDump,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d1bf7bf{/executors/threadDump/json,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d43a1b7{/static,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44aa91e2{/,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@650a1aff{/api,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11c7a0b4{/jobs/job/kill,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@653a5967{/stages/stage/kill,null,AVAILABLE,@Spark} 19/03/16 20:41:19 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.83.10:4041 19/03/16 20:41:19 INFO spark.SparkContext: Added JAR file:/root/mysql-connector-java-5.1.38.jar at spark://192.168.83.10:58142/jars/mysql-connector-java-5.1.38.jar with timestamp 1552740079770 19/03/16 20:41:20 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077... 19/03/16 20:41:20 INFO client.TransportClientFactory: Successfully created connection to master/192.168.83.10:7077 after 132 ms (0 ms spent in bootstraps) 19/03/16 20:41:40 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077... 19/03/16 20:42:00 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077... 19/03/16 20:42:20 ERROR cluster.StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up. 19/03/16 20:42:20 WARN cluster.StandaloneSchedulerBackend: Application ID is not initialized yet. 19/03/16 20:42:20 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34217. 19/03/16 20:42:20 INFO netty.NettyBlockTransferService: Server created on 192.168.83.10:34217 19/03/16 20:42:20 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy 19/03/16 20:42:20 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.83.10, 34217, None) 19/03/16 20:42:20 INFO server.AbstractConnector: Stopped Spark@2e49b91c{HTTP/1.1,[http/1.1]}{0.0.0.0:4041} 19/03/16 20:42:20 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.83.10:4041 19/03/16 20:42:20 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.83.10:34217 with 413.9 MB RAM, BlockManagerId(driver, 192.168.83.10, 34217, None) 19/03/16 20:42:20 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors 19/03/16 20:42:20 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.83.10, 34217, None) 19/03/16 20:42:20 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.83.10, 34217, None) 19/03/16 20:42:20 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down 19/03/16 20:42:20 WARN client.StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master 19/03/16 20:42:20 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! 19/03/16 20:42:21 INFO memory.MemoryStore: MemoryStore cleared 19/03/16 20:42:21 INFO storage.BlockManager: BlockManager stopped 19/03/16 20:42:21 INFO storage.BlockManagerMaster: BlockManagerMaster stopped 19/03/16 20:42:21 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped! 19/03/16 20:42:21 ERROR spark.SparkContext: Error initializing SparkContext. java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem at scala.Predef$.require(Predef.scala:224) at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:524) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901) at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:48) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.&lt;init&gt;(SparkSQLCLIDriver.scala:293) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:138) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 19/03/16 20:42:21 INFO spark.SparkContext: SparkContext already stopped. 19/03/16 20:42:21 INFO spark.SparkContext: Successfully stopped SparkContext Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem at scala.Predef$.require(Predef.scala:224) at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:524) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2509) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:909) at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:901) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901) at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:48) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.&lt;init&gt;(SparkSQLCLIDriver.scala:293) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:138) at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 19/03/16 20:42:21 INFO util.ShutdownHookManager: Shutdown hook called 19/03/16 20:42:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-06aa1468-390b-4e70-9151-405bd556805a 19/03/16 20:42:22 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6774f957-ed7c-4d8b-b3ce-eaf17bda1df1 问题原因 &amp; 解决方法 由于spark 配置为高可用模式下，要启动zookeeper 启动zookeeper后正常启动 [root@master sbin]# ./start-all.sh starting org.apache.spark.deploy.master.Master, logging to /opt/spark-2.2.0/logs/ ................. 日志量太大 此处省略 ........................... 19/03/16 20:47:52 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/opt/spark-2.2.0/sbin/spark-warehouse spark-sql&gt; ","link":"https://ibiji.top/post/1597299743021978/"},{"title":"hive启动客户端报错","content":"运行环境 | centos 7.0 | hive 1.2.2 | mysql-connector-java-5.1.38.jar 启动hive客户端报错 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ Connecting to jdbc:hive2://master:10000 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:279) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:206) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:507) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1612) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1630) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:551) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3000) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1107) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:873) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489) (state=,code=0) Beeline version 1.2.2 by Apache Hive Connecting to jdbc:hive2://master:10000 Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0) Beeline version 1.2.2 by Apache Hive 问题原因 &amp; 解决方法 hdfs 没有读写权限,配置文件中加入 &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; 对 hdfs /tmp 加入777 权限 hdfs dfs -chmod -R 777 /tmp/ ","link":"https://ibiji.top/post/1597299446220163/"},{"title":"hive客户端启动 Error Failed to open new session","content":"运行环境 | centos 6.6 | VMware Workstation Pro|hive-1.2.2|HADOOP 2.8.5 hive客户端连接hive服务端报错 Connecting to jdbc:hive2://master:10000 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException 服务端启动: nohup ./hiveserver2 1&gt;/root/hiveout.log 2&gt;&amp;1 &amp; 客户端连接： ./beeline -u jdbc:hive2://master:10000 启动后报错： Connecting to jdbc:hive2://master:10000 Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=anonymous, access=EXECUTE, inode=&quot;/tmp&quot;:root:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:279) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:206) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:507) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1612) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1630) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:551) at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3000) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1107) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:873) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489) (state=,code=0) Beeline version 1.2.2 by Apache Hive 在Hadoop core-site.xml 文件中加入(添加完需要重启服务)： &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; 在 hive-site.xml 中加入： &lt;property&gt; &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt; &lt;value&gt;777&lt;/value&gt; &lt;/property&gt; 再次启动 报错： Connecting to jdbc:hive2://master:10000 Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0) Beeline version 1.2.2 by Apache Hive 0: jdbc:hive2://master:10000 (closed)&gt; show databases; 对 hdfs /tmp添加 777 权限 hdfs dfs -chmod -R 777 /tmp/ 再次启动成功 [root@slave0 bin]# ./beeline -u jdbc:hive2://master:10000 Connecting to jdbc:hive2://master:10000 Connected to: Apache Hive (version 1.2.2) Driver: Hive JDBC (version 1.2.2) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 1.2.2 by Apache Hive 0: jdbc:hive2://master:10000&gt; show databases; +----------------+--+ | database_name | +----------------+--+ | default | | test_db | +----------------+--+ 2 rows selected (9.603 seconds) ","link":"https://ibiji.top/post/1597299390312556/"},{"title":"centos6.6启动报错 /dev/mapper/VolGroup-lv_root:UNEXPECTED......","content":"运行环境 | centos 6.6 | VMware Workstation Pro centos6.6启动报错/dev/mapper/VolGroup-lv_root:UNEXPECTED INCONSISTENCY;RUN fsck MANUALLY.(i.e., without -a or -p options) [FAILED]... 解决方案： 根据提示输入密码 输入：fsck -y /dev/root fsck -y /VolGroup-lv_root/root fsck -y /dev/sda1 中途会有提示输入y 重启后问题解决 fsck fsck（file system check）用来检查和维护不一致的文件系统。若系统掉电或磁盘发生问题，可利用fsck命令对文件系统进行检查。 使用方式 : fsck [-sACVRP] [-t fstype] [–] [fsck-options] filesys […] filesys ： 磁盘设备名称(eg./dev/sda1)，挂载（mount）点 (eg. / 或 /usr) -t : 给定档案系统的型式，若在 /etc/fstab 中已有定义或 kernel 本身已支援的则不需加上此参数 -s : 依序一个一个地执行 fsck 的指令来检查 -A : 对/etc/fstab 中所有列出来的 分区（partition）做检查 -C : 显示完整的检查进度 -d : 打印出 e2fsck 的 debug 结果 -p : 同时有 -A 条件时，同时有多个 fsck 的检查一起执行 -R : 同时有 -A 条件时，省略 / 不检查 -V : 详细显示模式 -a : 如果检查有错则自动修复 -r : 如果检查有错则由使用者回答是否修复 -y : 选项指定检测每个文件是自动输入yes，在不确定那些是不正常的时候，可以执行 # fsck -y 全部检查修复。 例子 : 检查 msdos 档案系统的 /dev/sda1 是否正常，如果有异常便自动修复 :fsck -t msdos -a /dev/sda1 ","link":"https://ibiji.top/post/1597298178642996/"},{"title":"eclipse多余工作空间记录删除方法","content":"运行环境 | windows | Eclipse Jee Latest Released 进入eclipse安装目录,在.settings 下 org.eclipse.ui.ide.prefs 文件 eclipse\\configuration\\.settings\\org.eclipse.ui.ide.prefs 第二行 每个记录以 \\n结束 ","link":"https://ibiji.top/post/1597298905899729/"},{"title":"comparator 排序报错IllegalArgumentException","content":"运行环境 | eclipse 12 | jdk1.8 | 数据量 100万 使用comparator 排序报错，十几条不出任何问题，正常排序，当超过100条数据时跑错java.lang.IllegalArgumentException 程序要对100万条json数据进行排序 java.lang.IllegalArgumentException: Comparison method violates its general contract! at java.util.TimSort.mergeLo(Unknown Source) at java.util.TimSort.mergeAt(Unknown Source) at java.util.TimSort.mergeCollapse(Unknown Source) at java.util.TimSort.sort(Unknown Source) at java.util.Arrays.sort(Unknown Source) at java.util.ArrayList.sort(Unknown Source) at java.util.Collections.sort(Unknown Source) at movieDemo.Demo4.main(Demo4.java:46) 解决方法： 需要判断 结果 等于0的情况 加上等于零的情况，正常排序 if(o1.getValue() == o2.getValue()) { return 0; } 或者使用三目运算嵌套 return o1.getValue() - o2.getValue() == 0 ? 0 : o1.getValue() - o2.getValue() &gt; 0 ?-1 : 1 ; 反例： return o1.getValue() - o2.getValue() &gt; 0 ? -1 : 1; ","link":"https://ibiji.top/post/1597298858993518/"},{"title":"rpc 报错 ClassNotFoundException","content":"运行环境 | eclipse 12 | jdk1.8 rpc发送Request对象报错 java.lang.ClassNotFoundException Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: rpc_client.pojo.Request at java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Unknown Source) at java.io.ObjectInputStream.resolveClass(Unknown Source) at java.io.ObjectInputStream.readNonProxyDesc(Unknown Source) at java.io.ObjectInputStream.readClassDesc(Unknown Source) at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source) at java.io.ObjectInputStream.readObject0(Unknown Source) at java.io.ObjectInputStream.readObject(Unknown Source) at rpc_server.admin.Server.main(Server.java:20) 解决方法： request 和 user 路径要一致（两文件路径必须一致） import rpc_client.pojo.Request; import rpc_client.pojo.User; 改为 import pojo.Request; import pojo.User; ","link":"https://ibiji.top/post/1597299715302788/"},{"title":"centos7 默认进入桌面和命令模式","content":"运行环境 | centos 7.0 systemctl get-default 查看当前的模式 [root@master ~]# systemctl get-default graphical.target systemctl set-default multi-user.target 设置默认进入 命令行模式 [root@master ~]# systemctl set-default multi-user.target Removed symlink /etc/systemd/system/default.target. Created symlink from /etc/systemd/system/default.target to /usr/lib/systemd/system/multi-user.target. ","link":"https://ibiji.top/post/1597298306520959/"},{"title":"启动 jenkins Job for jenkins service failed....","content":"搭建环境 | 系统：centos 7 | Java 1.8 | jenkins 2.121.3-1.1 2.1 [root@master ~]# /etc/init.d/jenkins start Starting jenkins (via systemctl): Job for jenkins.service failed because the control process exited with error code. See &quot;systemctl status jenkins.service&quot; and &quot;journalctl -xe&quot; for details. [FAILED] 查看日志 ● jenkins.service - LSB: Jenkins Automation Serve Loaded: loaded (/etc/rc.d/init.d/jenkins; bad; vendor preset: disabled) Active: failed (Result: exit-code) since Tue 2018-09-11 20:39:41 EDT; 2min 48s ago Docs: man:systemd-sysv-generator(8) Sep 11 20:39:38 master systemd[1]: Starting LSB: Jenkins Automation Server... Sep 11 20:39:40 master runuser[11110]: pam\\unix(runuser:session): session opened for ...=0) Sep 11 20:39:41 master jenkins[11104]: Starting Jenkins bash: /usr/jdk1.8: Is a directory Sep 11 20:39:41 master runuser[11110]: pam\\unix(runuser:session): session closed for ...oot Sep 11 20:39:41 master jenkins[11104]: [FAILED] Sep 11 20:39:41 master systemd[1]: jenkins.service: control process exited, code=exit...s=1 Sep 11 20:39:41 master systemd[1]: Failed to start LSB: Jenkins Automation Server. Sep 11 20:39:41 master systemd[1]: Unit jenkins.service entered failed state. Sep 11 20:39:41 master systemd[1]: jenkins.service failed. Hint: Some lines were ellipsized, use -l to show in full. 修改配置文件： candidates=“ #/etc/alternatives/java #/usr/lib/jvm/java-1.8.0/bin/java #/usr/lib/jvm/jre-1.8.0/bin/java #/usr/lib/jvm/java-1.7.0/bin/java #/usr/lib/jvm/jre-1.7.0/bin/java #/usr/bin/java #/usr/jdk1.8 /usr/jdk1.8/bin/java （注释掉原有jdk路径,添加自己的） 在candidates里插入自己的jdk java路径，其他的全部注释掉 #JENKINS\\_USER=&quot;jenkins&quot; JENKINS\\_USER=&quot;root&quot; JENKINS\\_USER 改成root 再次启动 jenkins 错误解决 [root@master ~]# /etc/init.d/jenkins start Starting jenkins (via systemctl): Warning: jenkins.service changed on disk. Run 'systemctl daemon-reload' to reload units. [ OK ] ","link":"https://ibiji.top/post/1597297914363211/"},{"title":"hive 初始化msyql报错 SQL Error code 1045","content":"搭建环境 | 系统：centos 7 | Hadoop：3.4.12 | hive：2.3.3 | mysql：5.6.40 | Java 1.8 msyql单独建立一个hive的用户的数据库，为了方便存hive的元数据 初始化mysql 报错： org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. Underlying cause: java.sql.SQLException : Access denied for user 'root'@'hive' (using password: YES) SQL Error code: 1045 Use --verbose for detailed stacktrace. *** schemaTool failed *** 按照百度教程，进mysql 修改了数据库的权限，不好使 最后用native for mysql挨个修改用户的密码，为了不出错，密码和root用户的密码保持一致，再次初始化 成功。 ","link":"https://ibiji.top/post/1597299356937235/"},{"title":"Hive初始化报错","content":"Hive初始化报错 org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:234) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) 完整报错信息 ls: 无法访问/opt/spark-2.2.0/lib/spark-assembly-*.jar: 没有那个文件或目录 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/hive-1.2.2/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/opt/hive-1.2.2/lib/hive-common-1.2.2.jar!/hive-log4j.properties Exception in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:234) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) ... 8 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) ... 14 more Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory NestedThrowables: java.lang.reflect.InvocationTargetException at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5768) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:199) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:74) ... 19 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) at org.datanucleus.store.AbstractStoreManager.&lt;init&gt;(AbstractStoreManager.java:240) at org.datanucleus.store.rdbms.RDBMSStoreManager.&lt;init&gt;(RDBMSStoreManager.java:286) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) ... 48 more Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the &quot;BONECP&quot; plugin to create a ConnectionPool gave an error : The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.&lt;init&gt;(ConnectionFactoryImpl.java:85) ... 66 more Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58) at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ... 68 more 原因：hivehome/lib下没有mysql jar包 放入不报错 ","link":"https://ibiji.top/post/1597299232891405/"},{"title":"centos7 安装mysql 及修改密码","content":"运行环境 | centos 7.0 | mysql 5.7 mysql安装 官网下载MySQL rpm包 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm rpm -ivh mysql57-community-release-el7-8.noarch.rpm yum install mysql-community-server 安装完后启动MySQL服务 service mysqld restart 方法一 查看默认密码 grep 'temporary password' /var/log/mysqld.log 设置密码： alter user 'root'@'localhost' identified by '新密码'; 如果报错 ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 修改密码强度 set global validate_password_policy=0; set global validate_password_length=1; 刷新后生效 mysql&gt; FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) 方法二 在 /etc/my.cnf 最后加上 skip-grant-tables 重启服务,免密码登陆，修改密码，刷新后生效 alter user 'root'@'localhost' identified by '新密码'; FLUSH PRIVILEGES; 创建用户密码规则修改 CREATE USER '用户'@'%' IDENTIFIED BY '密码'; 创建报错 ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 查看权限 select @@validate_password_policy; SHOW VARIABLES LIKE 'validate_password%'; Variable_name Value validate_password_check_user_name OFF validate_password_dictionary_file validate_password_length 8 validate_password_mixed_case_count 1 validate_password_number_count 1 validate_password_policy MEDIUM validate_password_special_char_count 1 修改密码等级 set global validate_password_policy=LOW; 设置密码长度 set global validate_password_length=5; 修改数据库远程登陆权限： use mysql; grant all privileges on *.* to 'root'@'%' identified by '密码' with grant option; 修改 AUTO_INCREMENT 值 alter table &lt;table name&gt; AUTO_INCREMENT=1; 错误解决 1、GPG 密钥 警告：/var/cache/yum/x86_64/7/mysql57-community/packages/mysql-community-client-5.7.38-1.el7.x86_64.rpm: 头V4 RSA/SHA256 Signature, 密钥 ID 3a79bd29: NOKEY 从 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 检索密钥 源 &quot;MySQL 5.7 Community Server&quot; 的 GPG 密钥已安装，但是不适用于此软件包。请检查源的公钥 URL 是否配置正确。 gpg --export -a 3a79bd29 &gt; 3a79bd29.asc rpm --import 3a79bd29.asc rpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022 ","link":"https://ibiji.top/post/1597298211916236/"},{"title":"centos7启动zookeeper 报错","content":"运行环境 | 操作系统:centos7 | jdk:1.8 | zookeeper：4.2.1 zookeeper 已经安装成功启动时报错。 运行 systemctl start zookeeper 后显示： Failed to start zookeeper.service: Unit not found. jps 也没有QuorumPeerMain进程 #zkServer.sh status 查看，反馈如下： #JMX enabled by default 检查配置文件 conf/zoo.cfg tickTime=2000 initLimit=5 syncLimit=2 dataDir=/opt/zookeeper/data dataLogDir=/opt/zookeeper/logs clientPort=2181 server.1=192.168.88.128:2888:3888 server.2=192.168.88.129:2888:3888 server.3=192.168.88.130:2888:3888 data/myid 1 三个节点中两个配置文件没有错误（myid文件分别为 1、2、 3） 检查 /etc/profile jdk和zookeeper的配置环境 #JDK1.8 configuration JAVA_HOME=/usr/jdk1.8 JRE_HOME=$ JAVA_HOME/jre PATH=$ PATH:$ JAVA_HOME/bin CLASSPATH=.: $ JAVA_HOME/lib/dt.jar:$ JAVA_HOME/lib/tools.jar export JAVA_HOME export JRE_HOME export PATH export CLASSPATH ZOOKEEPER_HOME=/opt/zookeeper export PATH= $ ZOOKEEPER_HOME/bin:$PATH export ZOOKEEPER_HOME export PATH 都没问题，最后是因为，没有产生环境变量 source下profile文件 在启动zookeeper启动成功。 ","link":"https://ibiji.top/post/1597298555199843/"},{"title":"hadoop 启动报错","content":"运行环境 | centos 7.0 | Hadoop 2.8.1 [root@master sbin]# ./start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [master] The authenticity of host 'master (192.168.91.10)' can't be established. ... RSA key fingerprint is 7e:3f:e7:5b:69:74:e3:0e:87:7b:2b:df:3d:64:b3:1e. Are you sure you want to continue connecting (yes/no)? yes master: Warning: Permanently added 'master,192.168.91.10' (RSA) to the list of known hosts. master: Error: JAVA_HOME is not set and could not be found. master: Error: JAVA_HOME is not set and could not be found. slave0: Error: JAVA_HOME is not set and could not be found. slave1: Error: JAVA_HOME is not set and could not be found. Starting secondary namenodes [0.0.0.0] The authenticity of host '0.0.0.0 (0.0.0.0)' can't be established. RSA key fingerprint is 7e:3f:e7:5b:69:74:e3:0e:87:7b:2b:df:3d:64:b3:1e. Are you sure you want to continue connecting (yes/no)? yes 0.0.0.0: Warning: Permanently added '0.0.0.0' (RSA) to the list of known hosts. 0.0.0.0: Error: JAVA_HOME is not set and could not be found. starting yarn daemons starting resourcemanager, logging to /opt/hadoop/logs/yarn-root-resourcemanager-master.out slave0: Error: JAVA_HOME is not set and could not be found. slave1: Error: JAVA_HOME is not set and could not be found. master: Error: JAVA_HOME is not set and could not be found. 查看各个配置文件 1.core-site.xml 2.hdfs-site.xml 3.mapred-site.xml 4.yarn-site.xml 修改 profile 文件 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:${HADOOP_HOME}/bin (这里要修改成自己的安装路径) 修改文件： [root@master hadoop]# vi hadoop-env.sh 文件中： export JAVA_HOME=${JAVA_HOME} 修改为（修改成自己的jdk路径）： export JAVA_HOME=/usr/jdk1.8 在重新启动,启动成功 查看进程,进程运行 错误解决 [root@master sbin]# ./start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [master] master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master. out slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave1. out master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master. out slave0: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0. out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-root-secondarynamenode-master. out starting yarn daemons resourcemanager running as process 3253. Stop it first. slave1: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave1. out master: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-master. out slave0: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave0.out [root@master sbin]# jps 4176 NodeManager 4289 Jps 3253 ResourceManager 3669 NameNode 3957 SecondaryNameNode 3771 DataNode``` ","link":"https://ibiji.top/post/1597299188850403/"}]}